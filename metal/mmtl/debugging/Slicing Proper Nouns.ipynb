{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all examples\n",
    " * `generate_uids=True`: return UIDs per example\n",
    " * `tokenizer=None`: return raw (untokenized) examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.mmtl.glue.glue_preprocess import load_tsv, get_task_tsv_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: In this case, it is easier to operate over the raw sentences and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_task_tsv_config('COLA', 'dev')\n",
    "    \n",
    "(sentences, labels), uids = load_tsv(\n",
    "    tsv_path=config[\"tsv_path\"],\n",
    "    sent1_idx=config[\"sent1_idx\"],\n",
    "    sent2_idx=config[\"sent2_idx\"],\n",
    "    label_idx=config[\"label_idx\"],\n",
    "    skip_rows=config[\"skip_rows\"],\n",
    "    delimiter=\"\\t\",\n",
    "    label_fn=config[\"label_fn\"],\n",
    "    generate_uids=True\n",
    ")\n",
    "\n",
    "assert len(sentences) == len(labels) == len(uids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Proper Nouns based on Entities\n",
    "Ref: https://spacy.io/api/annotation#named-entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('xx_ent_wiki_sm')\n",
    "\n",
    "def get_entities(sent, entities):\n",
    "    if sent is None:\n",
    "        return []\n",
    "\n",
    "    return [ent for ent in nlp(sent).ents if ent.label_ in entities]\n",
    "\n",
    "def ex_has_entities(ex, entities=[\"PER\", \"ORG\", \"LOC\"]):\n",
    "    # process sentence 1\n",
    "    proper_nouns = get_entities(ex[0], entities=entities)\n",
    "\n",
    "    if len(ex) == 2:\n",
    "        # process sentence 2\n",
    "        proper_nouns += get_entities(ex[0], entities=entities)\n",
    "\n",
    "    return len(proper_nouns) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag all examples in slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tagger import Tagger\n",
    "tagger = Tagger(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_name = 'proper_nouns'\n",
    "in_slice_fn = ex_has_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (ex, label, uid) in enumerate(zip(sentences, labels, uids)):   \n",
    "    \n",
    "    in_slice = in_slice_fn(ex)\n",
    "    \n",
    "    # logging for sanity check\n",
    "    if idx % 1000 == 0:\n",
    "        print((uid, ex, label), 'in_slice:', in_slice)\n",
    "        print()  \n",
    "\n",
    "    # if there are \"proper nouns\" as defined by entites, add the tag!\n",
    "    if in_slice:\n",
    "        tagger.add_tag(uid, slice_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_in_slice = len(tagger.get_uids(slice_name))\n",
    "num_ex = len(sentences)\n",
    "print(f\"% in slice ({num_in_slice}/{num_ex}) {num_in_slice/num_ex}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger.get_examples(slice_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval on slices with Uncased model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'COLA'\n",
    "model_path = '/dfs/scratch0/mccreery/mmtl/logs/ST_bertlarge/COLA/logdir/2019_02_25/COLA_21_56_02/best_model.pth'\n",
    "split = 'dev'\n",
    "bert_model = \"bert-large-uncased\"\n",
    "max_len = 200\n",
    "dl_kwargs = {\"shuffle\": False, \"batch_size\":1}\n",
    "\n",
    "tasks, payloads = create_glue_tasks_payloads(\n",
    "    task_names=[task_name],\n",
    "    bert_model=bert_model,\n",
    "    max_len=max_len,\n",
    "    dl_kwargs=dl_kwargs,\n",
    "    splits=[split],\n",
    "    max_datapoints=-1,\n",
    "    generate_uids=True,\n",
    ")\n",
    "\n",
    "model = MetalModel(tasks, verbose=False, device=0)\n",
    "dl = payloads[0].data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from metal.mmtl.debugging.utils import load_data_and_model, create_dataframe\n",
    "\n",
    "# Load model and data\n",
    "model_path = '/dfs/scratch0/mccreery/mmtl/logs/ST_bertlarge/COLA/logdir/2019_02_25/COLA_21_56_02/'\n",
    "task_name = 'COLA'\n",
    "split = 'dev'\n",
    "bert_model = \"bert-large-uncased\"\n",
    "model, dl = load_data_and_model(model_path, [task_name], split, bert_model=bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = f'{task_name}_{bert_model}_{split}_error_analysis.tsv'\n",
    "\n",
    "# Create DataFrame of Raw Data, Predictions, and Labels\n",
    "print('Creating dataframe')\n",
    "df_uncased = create_dataframe(task_name, model, dl, bert_model=bert_model)\n",
    "print('Created dataframe')\n",
    "\n",
    "# Save (and reload) DataFrame\n",
    "save_dataframe(df_uncased, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uncased = load_dataframe(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_for_uids(df, uids):\n",
    "    mask = df['uid'].apply(lambda x: x in uids)\n",
    "    return df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "slice_uids = tagger.get_uids(slice_name)\n",
    "df_uncased_in_slice = df_for_uids(df_uncased, slice_uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uncased_in_slice[df_uncased_in_slice['is_wrong']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Error rate:\", len(df_uncased_in_slice[df_uncased_in_slice['is_wrong']])\n",
    "                                              / len(df_uncased_in_slice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval on Slice with Cased Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.mmtl.debugging.utils import load_data_and_model, create_dataframe\n",
    "\n",
    "# Load model and data\n",
    "model_path = '/dfs/scratch0/mccreery/mmtl/logs/ST_bertlarge/COLA_cased/2/logdir/2019_03_05/COLA_00_50_04/best_model.pth'\n",
    "task_name = 'COLA'\n",
    "split = 'dev'\n",
    "bert_model = \"bert-large-cased\"\n",
    "model,dl = load_data_and_model(model_path, [task_name], split, bert_model=bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = f'{task_name}_{bert_model}_{split}_error_analysis.tsv'\n",
    "\n",
    "# Create DataFrame of Raw Data, Predictions, and Labels\n",
    "print('Creating dataframe')\n",
    "df_cased = create_dataframe(task_name, model, dl, bert_model=bert_model)\n",
    "print('Created dataframe')\n",
    "\n",
    "# Save (and reload) DataFrame\n",
    "save_dataframe(df_cased, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cased = load_dataframe(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cased_in_slice = df_for_uids(df_cased, slice_uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cased_in_slice[df_cased_in_slice['is_wrong']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Error rate:\", len(df_cased_in_slice[df_cased_in_slice['is_wrong']])\n",
    "                                              / len(df_cased_in_slice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis on differences in predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uncased.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cased.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve list uids corresponding to INCORRECT ex in UNCASED\n",
    "incorrect_uncased_uids = df_uncased_in_slice[df_uncased_in_slice['is_wrong']]['uid'].to_list()\n",
    "\n",
    "incorrect_cased_uids = df_cased_in_slice[df_cased_in_slice['is_wrong']]['uid'].to_list()\n",
    "\n",
    "# retrieved list of uids corresponding to CORRECT ex in CASED model\n",
    "correct_cased_uids = df_cased_in_slice[~df_cased_in_slice['is_wrong']]['uid'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which examples were \"corrected\" by the casing model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_by_casing_uids = set(incorrect_uncased_uids).intersection(set(correct_cased_uids))\n",
    "df_for_uids(df_uncased, corrected_by_casing_uids).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_uids(df_cased, corrected_by_casing_uids).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which examples are \"still incorrect\" with the casing model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "still_incorrect_uids = set(incorrect_uncased_uids).intersection(set(incorrect_cased_uids))\n",
    "df_for_uids(df_cased, still_incorrect_uids).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
