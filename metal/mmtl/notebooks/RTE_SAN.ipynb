{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from tqdm import tqdm\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "\n",
    "from metal.mmtl.glue_datasets import RTEDataset\n",
    "from metal.end_model import EndModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2490/2490 [00:03<00:00, 778.08it/s]\n",
      "100%|██████████| 277/277 [00:00<00:00, 622.80it/s]\n"
     ]
    }
   ],
   "source": [
    "model = 'bert-base-uncased' # also try bert-base-multilingual-cased (recommended)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "src_path = os.path.join(os.environ['GLUEDATA'], 'RTE/{}.tsv')\n",
    "dataset = {}\n",
    "for split in ['train', 'dev']:\n",
    "    dataset[split] = RTEDataset(src_path.format(split), tokenizer)\n",
    "    dataset[split].load_data()\n",
    "    dataset[split].preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['dev'].get_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for batch in dataset['dev'].get_dataloader(batch_size=2):\n",
    "#     (sent1, sent1_mask, sent2, sent2_mask), label = batch\n",
    "#     print(batch)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSelfAttn(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LinearSelfAttn, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "        self.softmax = nn.Softmax(1)\n",
    "\n",
    "    def forward(self, x, x_mask):\n",
    "        scores = self.linear(x).view(x.size(0), x.size(1))\n",
    "        scores.data.masked_fill_(x_mask.data, -float('inf'))\n",
    "        alpha = self.softmax(scores)\n",
    "        return alpha.unsqueeze(1).bmm(x).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilinearSelfAttn(nn.Module):\n",
    "    def __init__(self, x_size, y_size):\n",
    "        super(BilinearSelfAttn, self).__init__()\n",
    "        self.linear = nn.Linear(y_size, x_size)\n",
    "        self.softmax = nn.Softmax(1)\n",
    "\n",
    "    def forward(self, x, y, x_mask):\n",
    "        Wy = self.linear(y)\n",
    "        xWy = x.bmm(Wy.unsqueeze(2)).squeeze(2)\n",
    "        xWy.data.masked_fill_(x_mask.data, -float('inf'))\n",
    "        beta = self.softmax(xWy)\n",
    "        return beta.unsqueeze(1).bmm(x).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SAN(nn.Module):\n",
    "#     def __init__(self, emb_size=100, hidden_size=100, num_classes=2, k=5):\n",
    "#         super(SAN, self).__init__()\n",
    "#         self.bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "#         self.sent1_attn = LinearSelfAttn(input_size=emb_size)\n",
    "#         self.sent2_attn = BilinearSelfAttn(emb_size, emb_size)\n",
    "#         self.final_linear = nn.Linear(emb_size * 4, num_classes)\n",
    "#         self.rnn = rnn = nn.GRU(emb_size, hidden_size, 1, batch_first=True)\n",
    "#         self.k = k\n",
    "#         self.num_classes = num_classes\n",
    "#         self.softmax = nn.Softmax(1)\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         sent1, sent1_mask, sent2, sent2_mask = X\n",
    "#         batch_size = sent1.size(0)\n",
    "#         sent1, _ = self.bert_model(\n",
    "#             sent1, sent1_mask, 1 - sent1_mask, output_all_encoded_layers=False\n",
    "#         )\n",
    "#         sent2, _ = self.bert_model(\n",
    "#             sent2, sent2_mask, 1 - sent2_mask, output_all_encoded_layers=False\n",
    "#         )\n",
    "# #         print(sent1)\n",
    "# #         print(sent1.size())\n",
    "# #         print(sent2.size())\n",
    "#         outputs = []\n",
    "#         # sk (batch * embed_size)\n",
    "#         sk = self.sent1_attn(sent1, sent1_mask.byte())\n",
    "\n",
    "#         for i in range(self.k):\n",
    "#             xk = self.sent2_attn(sent2, sk, sent2_mask.byte())\n",
    "# #             print(sk.size(), xk.size())\n",
    "#             _, sk = self.rnn(xk.unsqueeze(1), sk.unsqueeze(0))\n",
    "#             sk = sk.squeeze(0)\n",
    "# #             print(sk.size())\n",
    "#             outputs.append((sk, xk))\n",
    "\n",
    "#         res = torch.zeros((batch_size, self.num_classes))\n",
    "\n",
    "#         for i in range(self.k):\n",
    "#             sk, xk = outputs[i]\n",
    "#             f = self.softmax(\n",
    "#                 self.final_linear(torch.cat((sk, xk, torch.abs(sk - xk), sk * xk), 1))\n",
    "#             )\n",
    "#             res += f\n",
    "#         return res / self.k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAN(nn.Module):\n",
    "    def __init__(self, emb_size=100, hidden_size=100, num_classes=2, k=5):\n",
    "        super(SAN, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.sent1_attn = LinearSelfAttn(input_size=emb_size)\n",
    "        self.sent2_attn = BilinearSelfAttn(emb_size, emb_size)\n",
    "        self.final_linear = nn.Linear(emb_size * 4, num_classes)\n",
    "        self.rnn = rnn = nn.GRU(emb_size, hidden_size, 1, batch_first=True)\n",
    "        self.k = k\n",
    "        self.num_classes = num_classes\n",
    "        self.softmax = nn.Softmax(1)\n",
    "        \n",
    "        for param in self.bert_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, X):\n",
    "#         print(\"!\")\n",
    "        sent1, sent1_mask, sent2, sent2_mask = X\n",
    "        batch_size = sent1.size(0)\n",
    "        sent1, _ = self.bert_model(\n",
    "            sent1, sent1_mask, 1 - sent1_mask, output_all_encoded_layers=False\n",
    "        )\n",
    "        sent2, _ = self.bert_model(\n",
    "            sent2, sent2_mask, 1 - sent2_mask, output_all_encoded_layers=False\n",
    "        )\n",
    "        res = sent1.new_zeros((batch_size, self.num_classes))\n",
    "#         res = torch.zeros((batch_size, self.num_classes))\n",
    "        sk = self.sent1_attn(sent1, sent1_mask.byte())\n",
    "\n",
    "        for i in range(self.k):\n",
    "            xk = self.sent2_attn(sent2, sk, sent2_mask.byte())\n",
    "            _, sk = self.rnn(xk.unsqueeze(1), sk.unsqueeze(0))\n",
    "            sk = sk.squeeze(0)\n",
    "\n",
    "            f = self.softmax(\n",
    "                self.final_linear(torch.cat((sk, xk, torch.abs(sk - xk), sk * xk), 1))\n",
    "            )\n",
    "            res += f\n",
    "\n",
    "        return res / self.k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "san = SAN(emb_size = 768, hidden_size = 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Network architecture:\n",
      "SAN(\n",
      "  (bert_model): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): BertLayerNorm()\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (sent1_attn): LinearSelfAttn(\n",
      "    (linear): Linear(in_features=768, out_features=1, bias=True)\n",
      "    (softmax): Softmax()\n",
      "  )\n",
      "  (sent2_attn): BilinearSelfAttn(\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (softmax): Softmax()\n",
      "  )\n",
      "  (final_linear): Linear(in_features=3072, out_features=2, bias=True)\n",
      "  (rnn): GRU(768, 768, batch_first=True)\n",
      "  (softmax): Softmax()\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "end_model = EndModel(\n",
    "    [2], input_module=san, seed=123, device=\"cuda\", skip_head=True, input_relu=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# end_model.train_model(\n",
    "#     dataset[\"train\"].get_dataloader(batch_size=10),\n",
    "#     valid_data=dataset[\"dev\"].get_dataloader(batch_size=10),\n",
    "# #     lr=5e-5\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU...\n",
      "[1 bat (0.00 epo)]: TRAIN:[loss=0.699] VALID:[accuracy=0.534]\n",
      "Saving model at iteration 1 with best score 0.534\n",
      "[2 bat (0.00 epo)]: TRAIN:[loss=0.699] VALID:[accuracy=0.534]\n",
      "[3 bat (0.00 epo)]: TRAIN:[loss=0.688] VALID:[accuracy=0.538]\n",
      "Saving model at iteration 3 with best score 0.538\n",
      "[4 bat (0.01 epo)]: TRAIN:[loss=0.699] VALID:[accuracy=0.505]\n",
      "[5 bat (0.01 epo)]: TRAIN:[loss=0.688] VALID:[accuracy=0.498]\n",
      "[6 bat (0.01 epo)]: TRAIN:[loss=0.686] VALID:[accuracy=0.505]\n",
      "[7 bat (0.01 epo)]: TRAIN:[loss=0.702] VALID:[accuracy=0.513]\n",
      "[8 bat (0.01 epo)]: TRAIN:[loss=0.705] VALID:[accuracy=0.505]\n",
      "[9 bat (0.01 epo)]: TRAIN:[loss=0.689] VALID:[accuracy=0.520]\n",
      "[10 bat (0.02 epo)]: TRAIN:[loss=0.700] VALID:[accuracy=0.513]\n",
      "[11 bat (0.02 epo)]: TRAIN:[loss=0.686] VALID:[accuracy=0.527]\n",
      "[12 bat (0.02 epo)]: TRAIN:[loss=0.691] VALID:[accuracy=0.534]\n",
      "[13 bat (0.02 epo)]: TRAIN:[loss=0.689] VALID:[accuracy=0.542]\n",
      "Saving model at iteration 13 with best score 0.542\n",
      "[14 bat (0.02 epo)]: TRAIN:[loss=0.693] VALID:[accuracy=0.542]\n",
      "[15 bat (0.02 epo)]: TRAIN:[loss=0.691] VALID:[accuracy=0.549]\n",
      "Saving model at iteration 15 with best score 0.549\n",
      "[16 bat (0.03 epo)]: TRAIN:[loss=0.704] VALID:[accuracy=0.542]\n",
      "[17 bat (0.03 epo)]: TRAIN:[loss=0.696] VALID:[accuracy=0.542]\n",
      "[18 bat (0.03 epo)]: TRAIN:[loss=0.691] VALID:[accuracy=0.552]\n",
      "Saving model at iteration 18 with best score 0.552\n",
      "[19 bat (0.03 epo)]: TRAIN:[loss=0.690] VALID:[accuracy=0.538]\n",
      "[20 bat (0.03 epo)]: TRAIN:[loss=0.681] VALID:[accuracy=0.534]\n",
      "[21 bat (0.03 epo)]: TRAIN:[loss=0.693] VALID:[accuracy=0.556]\n",
      "Saving model at iteration 21 with best score 0.556\n",
      "[22 bat (0.04 epo)]: TRAIN:[loss=0.688] VALID:[accuracy=0.549]\n",
      "[23 bat (0.04 epo)]: TRAIN:[loss=0.689] VALID:[accuracy=0.552]\n",
      "[24 bat (0.04 epo)]: TRAIN:[loss=0.692] VALID:[accuracy=0.552]\n",
      "[25 bat (0.04 epo)]: TRAIN:[loss=0.684] VALID:[accuracy=0.552]\n",
      "[26 bat (0.04 epo)]: TRAIN:[loss=0.706] VALID:[accuracy=0.549]\n",
      "[27 bat (0.04 epo)]: TRAIN:[loss=0.697] VALID:[accuracy=0.552]\n",
      "[28 bat (0.04 epo)]: TRAIN:[loss=0.673] VALID:[accuracy=0.552]\n",
      "[29 bat (0.05 epo)]: TRAIN:[loss=0.692] VALID:[accuracy=0.556]\n",
      "[30 bat (0.05 epo)]: TRAIN:[loss=0.670] VALID:[accuracy=0.563]\n",
      "Saving model at iteration 30 with best score 0.563\n",
      "[31 bat (0.05 epo)]: TRAIN:[loss=0.688] VALID:[accuracy=0.567]\n",
      "Saving model at iteration 31 with best score 0.567\n",
      "[32 bat (0.05 epo)]: TRAIN:[loss=0.705] VALID:[accuracy=0.567]\n",
      "[33 bat (0.05 epo)]: TRAIN:[loss=0.688] VALID:[accuracy=0.570]\n",
      "Saving model at iteration 33 with best score 0.570\n",
      "[34 bat (0.05 epo)]: TRAIN:[loss=0.688] VALID:[accuracy=0.567]\n",
      "[35 bat (0.06 epo)]: TRAIN:[loss=0.688] VALID:[accuracy=0.563]\n",
      "[36 bat (0.06 epo)]: TRAIN:[loss=0.694] VALID:[accuracy=0.563]\n",
      "[37 bat (0.06 epo)]: TRAIN:[loss=0.693] VALID:[accuracy=0.560]\n",
      "[38 bat (0.06 epo)]: TRAIN:[loss=0.703] VALID:[accuracy=0.549]\n",
      "[39 bat (0.06 epo)]: TRAIN:[loss=0.705] VALID:[accuracy=0.556]\n",
      "[40 bat (0.06 epo)]: TRAIN:[loss=0.700] VALID:[accuracy=0.560]\n",
      "[41 bat (0.07 epo)]: TRAIN:[loss=0.690] VALID:[accuracy=0.563]\n",
      "[42 bat (0.07 epo)]: TRAIN:[loss=0.701] VALID:[accuracy=0.552]\n",
      "[43 bat (0.07 epo)]: TRAIN:[loss=0.687] VALID:[accuracy=0.552]\n",
      "[44 bat (0.07 epo)]: TRAIN:[loss=0.678] VALID:[accuracy=0.527]\n",
      "[45 bat (0.07 epo)]: TRAIN:[loss=0.694] VALID:[accuracy=0.527]\n",
      "[46 bat (0.07 epo)]: TRAIN:[loss=0.682] VALID:[accuracy=0.516]\n",
      "[47 bat (0.08 epo)]: TRAIN:[loss=0.677] VALID:[accuracy=0.509]\n",
      "[48 bat (0.08 epo)]: TRAIN:[loss=0.693] VALID:[accuracy=0.520]\n",
      "[49 bat (0.08 epo)]: TRAIN:[loss=0.701] VALID:[accuracy=0.520]\n",
      "[50 bat (0.08 epo)]: TRAIN:[loss=0.689] VALID:[accuracy=0.523]\n",
      "[51 bat (0.08 epo)]: TRAIN:[loss=0.685] VALID:[accuracy=0.527]\n",
      "[52 bat (0.08 epo)]: TRAIN:[loss=0.687] VALID:[accuracy=0.523]\n",
      "[53 bat (0.09 epo)]: TRAIN:[loss=0.704] VALID:[accuracy=0.527]\n",
      "[54 bat (0.09 epo)]: TRAIN:[loss=0.696] VALID:[accuracy=0.516]\n",
      "[55 bat (0.09 epo)]: TRAIN:[loss=0.681] VALID:[accuracy=0.498]\n",
      "[56 bat (0.09 epo)]: TRAIN:[loss=0.711] VALID:[accuracy=0.484]\n",
      "[57 bat (0.09 epo)]: TRAIN:[loss=0.695] VALID:[accuracy=0.491]\n",
      "[58 bat (0.09 epo)]: TRAIN:[loss=0.693] VALID:[accuracy=0.484]\n",
      "[59 bat (0.09 epo)]: TRAIN:[loss=0.698] VALID:[accuracy=0.477]\n",
      "[60 bat (0.10 epo)]: TRAIN:[loss=0.693] VALID:[accuracy=0.480]\n",
      "[61 bat (0.10 epo)]: TRAIN:[loss=0.720] VALID:[accuracy=0.480]\n",
      "[62 bat (0.10 epo)]: TRAIN:[loss=0.706] VALID:[accuracy=0.487]\n",
      "[63 bat (0.10 epo)]: TRAIN:[loss=0.693] VALID:[accuracy=0.484]\n",
      "[64 bat (0.10 epo)]: TRAIN:[loss=0.694] VALID:[accuracy=0.487]\n",
      "[65 bat (0.10 epo)]: TRAIN:[loss=0.692] VALID:[accuracy=0.484]\n",
      "[66 bat (0.11 epo)]: TRAIN:[loss=0.681] VALID:[accuracy=0.487]\n",
      "[67 bat (0.11 epo)]: TRAIN:[loss=0.700] VALID:[accuracy=0.487]\n",
      "[68 bat (0.11 epo)]: TRAIN:[loss=0.682] VALID:[accuracy=0.487]\n",
      "[69 bat (0.11 epo)]: TRAIN:[loss=0.688] VALID:[accuracy=0.495]\n",
      "[70 bat (0.11 epo)]: TRAIN:[loss=0.693] VALID:[accuracy=0.495]\n",
      "[71 bat (0.11 epo)]: TRAIN:[loss=0.695] VALID:[accuracy=0.498]\n",
      "[72 bat (0.12 epo)]: TRAIN:[loss=0.707] VALID:[accuracy=0.516]\n",
      "[73 bat (0.12 epo)]: TRAIN:[loss=0.683] VALID:[accuracy=0.523]\n",
      "[74 bat (0.12 epo)]: TRAIN:[loss=0.713] VALID:[accuracy=0.531]\n",
      "[75 bat (0.12 epo)]: TRAIN:[loss=0.692] VALID:[accuracy=0.523]\n",
      "[76 bat (0.12 epo)]: TRAIN:[loss=0.695] VALID:[accuracy=0.534]\n",
      "[77 bat (0.12 epo)]: TRAIN:[loss=0.699] VALID:[accuracy=0.560]\n",
      "[78 bat (0.13 epo)]: TRAIN:[loss=0.678] VALID:[accuracy=0.556]\n",
      "[79 bat (0.13 epo)]: TRAIN:[loss=0.687] VALID:[accuracy=0.563]\n",
      "[80 bat (0.13 epo)]: TRAIN:[loss=0.702] VALID:[accuracy=0.563]\n",
      "[81 bat (0.13 epo)]: TRAIN:[loss=0.683] VALID:[accuracy=0.570]\n",
      "[82 bat (0.13 epo)]: TRAIN:[loss=0.690] VALID:[accuracy=0.578]\n",
      "Saving model at iteration 82 with best score 0.578\n",
      "[83 bat (0.13 epo)]: TRAIN:[loss=0.701] VALID:[accuracy=0.574]\n",
      "[84 bat (0.13 epo)]: TRAIN:[loss=0.672] VALID:[accuracy=0.574]\n",
      "[85 bat (0.14 epo)]: TRAIN:[loss=0.687] VALID:[accuracy=0.574]\n",
      "[86 bat (0.14 epo)]: TRAIN:[loss=0.673] VALID:[accuracy=0.570]\n",
      "[87 bat (0.14 epo)]: TRAIN:[loss=0.677] VALID:[accuracy=0.570]\n",
      "[88 bat (0.14 epo)]: TRAIN:[loss=0.685] VALID:[accuracy=0.567]\n",
      "[89 bat (0.14 epo)]: TRAIN:[loss=0.690] VALID:[accuracy=0.560]\n",
      "[90 bat (0.14 epo)]: TRAIN:[loss=0.698] VALID:[accuracy=0.552]\n",
      "[91 bat (0.15 epo)]: TRAIN:[loss=0.677] VALID:[accuracy=0.549]\n",
      "[92 bat (0.15 epo)]: TRAIN:[loss=0.668] VALID:[accuracy=0.538]\n",
      "[93 bat (0.15 epo)]: TRAIN:[loss=0.693] VALID:[accuracy=0.542]\n",
      "[94 bat (0.15 epo)]: TRAIN:[loss=0.690] VALID:[accuracy=0.538]\n",
      "[95 bat (0.15 epo)]: TRAIN:[loss=0.685] VALID:[accuracy=0.552]\n",
      "[96 bat (0.15 epo)]: TRAIN:[loss=0.681] VALID:[accuracy=0.545]\n",
      "[97 bat (0.16 epo)]: TRAIN:[loss=0.698] VALID:[accuracy=0.545]\n",
      "[98 bat (0.16 epo)]: TRAIN:[loss=0.699] VALID:[accuracy=0.560]\n",
      "[99 bat (0.16 epo)]: TRAIN:[loss=0.694] VALID:[accuracy=0.560]\n",
      "[100 bat (0.16 epo)]: TRAIN:[loss=0.709] VALID:[accuracy=0.567]\n",
      "[101 bat (0.16 epo)]: TRAIN:[loss=0.702] VALID:[accuracy=0.545]\n",
      "[102 bat (0.16 epo)]: TRAIN:[loss=0.693] VALID:[accuracy=0.534]\n",
      "[103 bat (0.17 epo)]: TRAIN:[loss=0.678] VALID:[accuracy=0.520]\n",
      "[104 bat (0.17 epo)]: TRAIN:[loss=0.697] VALID:[accuracy=0.523]\n",
      "[105 bat (0.17 epo)]: TRAIN:[loss=0.657] VALID:[accuracy=0.527]\n",
      "[106 bat (0.17 epo)]: TRAIN:[loss=0.709] VALID:[accuracy=0.534]\n",
      "[107 bat (0.17 epo)]: TRAIN:[loss=0.700] VALID:[accuracy=0.542]\n",
      "[108 bat (0.17 epo)]: TRAIN:[loss=0.694] VALID:[accuracy=0.538]\n",
      "[109 bat (0.18 epo)]: TRAIN:[loss=0.704] VALID:[accuracy=0.542]\n",
      "[110 bat (0.18 epo)]: TRAIN:[loss=0.701] VALID:[accuracy=0.538]\n",
      "[111 bat (0.18 epo)]: TRAIN:[loss=0.691] VALID:[accuracy=0.542]\n",
      "[112 bat (0.18 epo)]: TRAIN:[loss=0.699] VALID:[accuracy=0.531]\n",
      "[113 bat (0.18 epo)]: TRAIN:[loss=0.668] VALID:[accuracy=0.516]\n",
      "[114 bat (0.18 epo)]: TRAIN:[loss=0.676] VALID:[accuracy=0.495]\n",
      "[115 bat (0.18 epo)]: TRAIN:[loss=0.676] VALID:[accuracy=0.480]\n",
      "[116 bat (0.19 epo)]: TRAIN:[loss=0.694] VALID:[accuracy=0.473]\n",
      "[117 bat (0.19 epo)]: TRAIN:[loss=0.695] VALID:[accuracy=0.473]\n",
      "[118 bat (0.19 epo)]: TRAIN:[loss=0.736] VALID:[accuracy=0.477]\n",
      "[119 bat (0.19 epo)]: TRAIN:[loss=0.703] VALID:[accuracy=0.473]\n",
      "[120 bat (0.19 epo)]: TRAIN:[loss=0.712] VALID:[accuracy=0.473]\n",
      "[121 bat (0.19 epo)]: TRAIN:[loss=0.699] VALID:[accuracy=0.477]\n",
      "[122 bat (0.20 epo)]: TRAIN:[loss=0.691] VALID:[accuracy=0.477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[123 bat (0.20 epo)]: TRAIN:[loss=0.679] VALID:[accuracy=0.491]\n",
      "[124 bat (0.20 epo)]: TRAIN:[loss=0.686] VALID:[accuracy=0.505]\n",
      "[125 bat (0.20 epo)]: TRAIN:[loss=0.686] VALID:[accuracy=0.513]\n",
      "[126 bat (0.20 epo)]: TRAIN:[loss=0.665] VALID:[accuracy=0.513]\n",
      "[127 bat (0.20 epo)]: TRAIN:[loss=0.688] VALID:[accuracy=0.509]\n",
      "[128 bat (0.21 epo)]: TRAIN:[loss=0.691] VALID:[accuracy=0.516]\n",
      "[129 bat (0.21 epo)]: TRAIN:[loss=0.688] VALID:[accuracy=0.531]\n",
      "[130 bat (0.21 epo)]: TRAIN:[loss=0.683] VALID:[accuracy=0.538]\n",
      "[131 bat (0.21 epo)]: TRAIN:[loss=0.679] VALID:[accuracy=0.534]\n",
      "[132 bat (0.21 epo)]: TRAIN:[loss=0.699] VALID:[accuracy=0.527]\n",
      "[133 bat (0.21 epo)]: TRAIN:[loss=0.667] VALID:[accuracy=0.531]\n",
      "[134 bat (0.22 epo)]: TRAIN:[loss=0.694] VALID:[accuracy=0.513]\n",
      "[135 bat (0.22 epo)]: TRAIN:[loss=0.685] VALID:[accuracy=0.531]\n",
      "[136 bat (0.22 epo)]: TRAIN:[loss=0.674] VALID:[accuracy=0.563]\n",
      "[137 bat (0.22 epo)]: TRAIN:[loss=0.685] VALID:[accuracy=0.570]\n",
      "[138 bat (0.22 epo)]: TRAIN:[loss=0.697] VALID:[accuracy=0.563]\n",
      "[139 bat (0.22 epo)]: TRAIN:[loss=0.680] VALID:[accuracy=0.588]\n",
      "Saving model at iteration 139 with best score 0.588\n",
      "[140 bat (0.22 epo)]: TRAIN:[loss=0.686] VALID:[accuracy=0.585]\n",
      "[141 bat (0.23 epo)]: TRAIN:[loss=0.707] VALID:[accuracy=0.592]\n",
      "Saving model at iteration 141 with best score 0.592\n",
      "[142 bat (0.23 epo)]: TRAIN:[loss=0.683] VALID:[accuracy=0.592]\n",
      "[143 bat (0.23 epo)]: TRAIN:[loss=0.696] VALID:[accuracy=0.592]\n",
      "[144 bat (0.23 epo)]: TRAIN:[loss=0.664] VALID:[accuracy=0.592]\n",
      "[145 bat (0.23 epo)]: TRAIN:[loss=0.667] VALID:[accuracy=0.585]\n",
      "[146 bat (0.23 epo)]: TRAIN:[loss=0.701] VALID:[accuracy=0.581]\n",
      "[147 bat (0.24 epo)]: TRAIN:[loss=0.682] VALID:[accuracy=0.585]\n",
      "[148 bat (0.24 epo)]: TRAIN:[loss=0.700] VALID:[accuracy=0.585]\n",
      "[149 bat (0.24 epo)]: TRAIN:[loss=0.675] VALID:[accuracy=0.592]\n",
      "[150 bat (0.24 epo)]: TRAIN:[loss=0.706] VALID:[accuracy=0.596]\n",
      "Saving model at iteration 150 with best score 0.596\n",
      "[151 bat (0.24 epo)]: TRAIN:[loss=0.688] VALID:[accuracy=0.588]\n",
      "[152 bat (0.24 epo)]: TRAIN:[loss=0.689] VALID:[accuracy=0.581]\n",
      "[153 bat (0.25 epo)]: TRAIN:[loss=0.679] VALID:[accuracy=0.581]\n",
      "[154 bat (0.25 epo)]: TRAIN:[loss=0.715] VALID:[accuracy=0.585]\n",
      "[155 bat (0.25 epo)]: TRAIN:[loss=0.704] VALID:[accuracy=0.588]\n",
      "[156 bat (0.25 epo)]: TRAIN:[loss=0.675] VALID:[accuracy=0.585]\n",
      "[157 bat (0.25 epo)]: TRAIN:[loss=0.704] VALID:[accuracy=0.581]\n",
      "[158 bat (0.25 epo)]: TRAIN:[loss=0.707] VALID:[accuracy=0.560]\n",
      "[159 bat (0.26 epo)]: TRAIN:[loss=0.689] VALID:[accuracy=0.552]\n",
      "[160 bat (0.26 epo)]: TRAIN:[loss=0.701] VALID:[accuracy=0.538]\n",
      "[161 bat (0.26 epo)]: TRAIN:[loss=0.695] VALID:[accuracy=0.542]\n",
      "[162 bat (0.26 epo)]: TRAIN:[loss=0.676] VALID:[accuracy=0.531]\n",
      "[163 bat (0.26 epo)]: TRAIN:[loss=0.703] VALID:[accuracy=0.531]\n",
      "[164 bat (0.26 epo)]: TRAIN:[loss=0.670] VALID:[accuracy=0.534]\n",
      "[165 bat (0.27 epo)]: TRAIN:[loss=0.674] VALID:[accuracy=0.538]\n",
      "[166 bat (0.27 epo)]: TRAIN:[loss=0.690] VALID:[accuracy=0.534]\n",
      "[167 bat (0.27 epo)]: TRAIN:[loss=0.677] VALID:[accuracy=0.531]\n",
      "[168 bat (0.27 epo)]: TRAIN:[loss=0.685] VALID:[accuracy=0.538]\n",
      "[169 bat (0.27 epo)]: TRAIN:[loss=0.678] VALID:[accuracy=0.542]\n",
      "[170 bat (0.27 epo)]: TRAIN:[loss=0.707] VALID:[accuracy=0.545]\n",
      "[171 bat (0.27 epo)]: TRAIN:[loss=0.698] VALID:[accuracy=0.542]\n",
      "[172 bat (0.28 epo)]: TRAIN:[loss=0.669] VALID:[accuracy=0.542]\n",
      "[173 bat (0.28 epo)]: TRAIN:[loss=0.695] VALID:[accuracy=0.545]\n",
      "[174 bat (0.28 epo)]: TRAIN:[loss=0.668] VALID:[accuracy=0.542]\n",
      "[175 bat (0.28 epo)]: TRAIN:[loss=0.694] VALID:[accuracy=0.549]\n",
      "[176 bat (0.28 epo)]: TRAIN:[loss=0.682] VALID:[accuracy=0.556]\n",
      "[177 bat (0.28 epo)]: TRAIN:[loss=0.680] VALID:[accuracy=0.552]\n",
      "[178 bat (0.29 epo)]: TRAIN:[loss=0.670] VALID:[accuracy=0.552]\n",
      "[179 bat (0.29 epo)]: TRAIN:[loss=0.712] VALID:[accuracy=0.556]\n",
      "[180 bat (0.29 epo)]: TRAIN:[loss=0.700] VALID:[accuracy=0.552]\n",
      "[181 bat (0.29 epo)]: TRAIN:[loss=0.661] VALID:[accuracy=0.552]\n",
      "[182 bat (0.29 epo)]: TRAIN:[loss=0.680] VALID:[accuracy=0.542]\n",
      "[183 bat (0.29 epo)]: TRAIN:[loss=0.677] VALID:[accuracy=0.542]\n",
      "[184 bat (0.30 epo)]: TRAIN:[loss=0.699] VALID:[accuracy=0.542]\n",
      "[185 bat (0.30 epo)]: TRAIN:[loss=0.708] VALID:[accuracy=0.534]\n",
      "[186 bat (0.30 epo)]: TRAIN:[loss=0.674] VALID:[accuracy=0.531]\n",
      "[187 bat (0.30 epo)]: TRAIN:[loss=0.651] VALID:[accuracy=0.531]\n",
      "[188 bat (0.30 epo)]: TRAIN:[loss=0.681] VALID:[accuracy=0.531]\n",
      "[189 bat (0.30 epo)]: TRAIN:[loss=0.674] VALID:[accuracy=0.516]\n",
      "[190 bat (0.31 epo)]: TRAIN:[loss=0.691] VALID:[accuracy=0.513]\n",
      "[191 bat (0.31 epo)]: TRAIN:[loss=0.661] VALID:[accuracy=0.542]\n",
      "[192 bat (0.31 epo)]: TRAIN:[loss=0.700] VALID:[accuracy=0.542]\n",
      "[193 bat (0.31 epo)]: TRAIN:[loss=0.687] VALID:[accuracy=0.534]\n",
      "[194 bat (0.31 epo)]: TRAIN:[loss=0.675] VALID:[accuracy=0.534]\n",
      "[195 bat (0.31 epo)]: TRAIN:[loss=0.700] VALID:[accuracy=0.534]\n",
      "[196 bat (0.31 epo)]: TRAIN:[loss=0.712] VALID:[accuracy=0.534]\n",
      "[197 bat (0.32 epo)]: TRAIN:[loss=0.690] VALID:[accuracy=0.531]\n",
      "[198 bat (0.32 epo)]: TRAIN:[loss=0.679] VALID:[accuracy=0.534]\n",
      "[199 bat (0.32 epo)]: TRAIN:[loss=0.686] VALID:[accuracy=0.538]\n",
      "[200 bat (0.32 epo)]: TRAIN:[loss=0.684] VALID:[accuracy=0.542]\n",
      "[201 bat (0.32 epo)]: TRAIN:[loss=0.695] VALID:[accuracy=0.542]\n",
      "[202 bat (0.32 epo)]: TRAIN:[loss=0.702] VALID:[accuracy=0.534]\n",
      "[203 bat (0.33 epo)]: TRAIN:[loss=0.651] VALID:[accuracy=0.527]\n",
      "[204 bat (0.33 epo)]: TRAIN:[loss=0.682] VALID:[accuracy=0.531]\n",
      "[205 bat (0.33 epo)]: TRAIN:[loss=0.669] VALID:[accuracy=0.531]\n",
      "[206 bat (0.33 epo)]: TRAIN:[loss=0.700] VALID:[accuracy=0.538]\n",
      "[207 bat (0.33 epo)]: TRAIN:[loss=0.694] VALID:[accuracy=0.552]\n",
      "[208 bat (0.33 epo)]: TRAIN:[loss=0.704] VALID:[accuracy=0.549]\n",
      "[209 bat (0.34 epo)]: TRAIN:[loss=0.655] VALID:[accuracy=0.560]\n",
      "[210 bat (0.34 epo)]: TRAIN:[loss=0.673] VALID:[accuracy=0.556]\n",
      "[211 bat (0.34 epo)]: TRAIN:[loss=0.689] VALID:[accuracy=0.560]\n",
      "[212 bat (0.34 epo)]: TRAIN:[loss=0.678] VALID:[accuracy=0.585]\n",
      "[213 bat (0.34 epo)]: TRAIN:[loss=0.674] VALID:[accuracy=0.574]\n",
      "[214 bat (0.34 epo)]: TRAIN:[loss=0.666] VALID:[accuracy=0.560]\n",
      "[215 bat (0.35 epo)]: TRAIN:[loss=0.676] VALID:[accuracy=0.581]\n",
      "[216 bat (0.35 epo)]: TRAIN:[loss=0.692] VALID:[accuracy=0.578]\n",
      "[217 bat (0.35 epo)]: TRAIN:[loss=0.686] VALID:[accuracy=0.578]\n",
      "[218 bat (0.35 epo)]: TRAIN:[loss=0.657] VALID:[accuracy=0.574]\n",
      "[219 bat (0.35 epo)]: TRAIN:[loss=0.689] VALID:[accuracy=0.574]\n",
      "[220 bat (0.35 epo)]: TRAIN:[loss=0.703] VALID:[accuracy=0.574]\n",
      "[221 bat (0.36 epo)]: TRAIN:[loss=0.679] VALID:[accuracy=0.574]\n",
      "[222 bat (0.36 epo)]: TRAIN:[loss=0.682] VALID:[accuracy=0.578]\n",
      "[223 bat (0.36 epo)]: TRAIN:[loss=0.674] VALID:[accuracy=0.574]\n",
      "[224 bat (0.36 epo)]: TRAIN:[loss=0.697] VALID:[accuracy=0.574]\n",
      "[225 bat (0.36 epo)]: TRAIN:[loss=0.682] VALID:[accuracy=0.574]\n",
      "[226 bat (0.36 epo)]: TRAIN:[loss=0.706] VALID:[accuracy=0.578]\n",
      "[227 bat (0.36 epo)]: TRAIN:[loss=0.705] VALID:[accuracy=0.578]\n",
      "[228 bat (0.37 epo)]: TRAIN:[loss=0.699] VALID:[accuracy=0.578]\n",
      "[229 bat (0.37 epo)]: TRAIN:[loss=0.685] VALID:[accuracy=0.574]\n",
      "[230 bat (0.37 epo)]: TRAIN:[loss=0.725] VALID:[accuracy=0.574]\n",
      "[231 bat (0.37 epo)]: TRAIN:[loss=0.701] VALID:[accuracy=0.552]\n",
      "[232 bat (0.37 epo)]: TRAIN:[loss=0.666] VALID:[accuracy=0.552]\n",
      "[233 bat (0.37 epo)]: TRAIN:[loss=0.682] VALID:[accuracy=0.563]\n",
      "[234 bat (0.38 epo)]: TRAIN:[loss=0.689] VALID:[accuracy=0.567]\n",
      "[235 bat (0.38 epo)]: TRAIN:[loss=0.701] VALID:[accuracy=0.542]\n",
      "[236 bat (0.38 epo)]: TRAIN:[loss=0.668] VALID:[accuracy=0.549]\n",
      "[237 bat (0.38 epo)]: TRAIN:[loss=0.691] VALID:[accuracy=0.542]\n",
      "[238 bat (0.38 epo)]: TRAIN:[loss=0.661] VALID:[accuracy=0.542]\n",
      "[239 bat (0.38 epo)]: TRAIN:[loss=0.678] VALID:[accuracy=0.538]\n",
      "[240 bat (0.39 epo)]: TRAIN:[loss=0.692] VALID:[accuracy=0.538]\n",
      "[241 bat (0.39 epo)]: TRAIN:[loss=0.689] VALID:[accuracy=0.538]\n",
      "[242 bat (0.39 epo)]: TRAIN:[loss=0.688] VALID:[accuracy=0.538]\n",
      "[243 bat (0.39 epo)]: TRAIN:[loss=0.685] VALID:[accuracy=0.538]\n",
      "[244 bat (0.39 epo)]: TRAIN:[loss=0.672] VALID:[accuracy=0.545]\n",
      "[245 bat (0.39 epo)]: TRAIN:[loss=0.681] VALID:[accuracy=0.545]\n",
      "[246 bat (0.40 epo)]: TRAIN:[loss=0.649] VALID:[accuracy=0.549]\n",
      "[247 bat (0.40 epo)]: TRAIN:[loss=0.688] VALID:[accuracy=0.549]\n",
      "[248 bat (0.40 epo)]: TRAIN:[loss=0.674] VALID:[accuracy=0.534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[249 bat (0.40 epo)]: TRAIN:[loss=0.702] VALID:[accuracy=0.542]\n",
      "[250 bat (0.40 epo)]: TRAIN:[loss=0.682] VALID:[accuracy=0.549]\n",
      "[251 bat (0.40 epo)]: TRAIN:[loss=0.666] VALID:[accuracy=0.563]\n",
      "[252 bat (0.40 epo)]: TRAIN:[loss=0.683] VALID:[accuracy=0.567]\n",
      "[253 bat (0.41 epo)]: TRAIN:[loss=0.684] VALID:[accuracy=0.570]\n",
      "[254 bat (0.41 epo)]: TRAIN:[loss=0.670] VALID:[accuracy=0.574]\n",
      "[255 bat (0.41 epo)]: TRAIN:[loss=0.715] VALID:[accuracy=0.574]\n",
      "[256 bat (0.41 epo)]: TRAIN:[loss=0.686] VALID:[accuracy=0.574]\n",
      "[257 bat (0.41 epo)]: TRAIN:[loss=0.703] VALID:[accuracy=0.570]\n",
      "[258 bat (0.41 epo)]: TRAIN:[loss=0.685] VALID:[accuracy=0.567]\n",
      "[259 bat (0.42 epo)]: TRAIN:[loss=0.667] VALID:[accuracy=0.560]\n",
      "[260 bat (0.42 epo)]: TRAIN:[loss=0.728] VALID:[accuracy=0.545]\n",
      "[261 bat (0.42 epo)]: TRAIN:[loss=0.682] VALID:[accuracy=0.538]\n",
      "[262 bat (0.42 epo)]: TRAIN:[loss=0.676] VALID:[accuracy=0.542]\n",
      "[263 bat (0.42 epo)]: TRAIN:[loss=0.671] VALID:[accuracy=0.545]\n",
      "[264 bat (0.42 epo)]: TRAIN:[loss=0.705] VALID:[accuracy=0.549]\n",
      "[265 bat (0.43 epo)]: TRAIN:[loss=0.694] VALID:[accuracy=0.549]\n",
      "[266 bat (0.43 epo)]: TRAIN:[loss=0.688] VALID:[accuracy=0.549]\n",
      "[267 bat (0.43 epo)]: TRAIN:[loss=0.656] VALID:[accuracy=0.549]\n",
      "[268 bat (0.43 epo)]: TRAIN:[loss=0.685] VALID:[accuracy=0.549]\n",
      "[269 bat (0.43 epo)]: TRAIN:[loss=0.683] VALID:[accuracy=0.549]\n",
      "[270 bat (0.43 epo)]: TRAIN:[loss=0.673] VALID:[accuracy=0.542]\n",
      "[271 bat (0.44 epo)]: TRAIN:[loss=0.706] VALID:[accuracy=0.531]\n",
      "[272 bat (0.44 epo)]: TRAIN:[loss=0.696] VALID:[accuracy=0.542]\n",
      "[273 bat (0.44 epo)]: TRAIN:[loss=0.679] VALID:[accuracy=0.563]\n",
      "[274 bat (0.44 epo)]: TRAIN:[loss=0.681] VALID:[accuracy=0.567]\n",
      "[275 bat (0.44 epo)]: TRAIN:[loss=0.689] VALID:[accuracy=0.552]\n",
      "[276 bat (0.44 epo)]: TRAIN:[loss=0.705] VALID:[accuracy=0.549]\n",
      "[277 bat (0.44 epo)]: TRAIN:[loss=0.671] VALID:[accuracy=0.563]\n",
      "[278 bat (0.45 epo)]: TRAIN:[loss=0.703] VALID:[accuracy=0.563]\n",
      "[279 bat (0.45 epo)]: TRAIN:[loss=0.660] VALID:[accuracy=0.556]\n",
      "[280 bat (0.45 epo)]: TRAIN:[loss=0.708] VALID:[accuracy=0.556]\n",
      "[281 bat (0.45 epo)]: TRAIN:[loss=0.681] VALID:[accuracy=0.560]\n",
      "[282 bat (0.45 epo)]: TRAIN:[loss=0.672] VALID:[accuracy=0.563]\n",
      "[283 bat (0.45 epo)]: TRAIN:[loss=0.699] VALID:[accuracy=0.556]\n",
      "[284 bat (0.46 epo)]: TRAIN:[loss=0.688] VALID:[accuracy=0.560]\n",
      "[285 bat (0.46 epo)]: TRAIN:[loss=0.688] VALID:[accuracy=0.560]\n",
      "[286 bat (0.46 epo)]: TRAIN:[loss=0.691] VALID:[accuracy=0.567]\n",
      "[287 bat (0.46 epo)]: TRAIN:[loss=0.670] VALID:[accuracy=0.560]\n",
      "[288 bat (0.46 epo)]: TRAIN:[loss=0.664] VALID:[accuracy=0.556]\n",
      "[289 bat (0.46 epo)]: TRAIN:[loss=0.700] VALID:[accuracy=0.545]\n",
      "[290 bat (0.47 epo)]: TRAIN:[loss=0.696] VALID:[accuracy=0.542]\n",
      "[291 bat (0.47 epo)]: TRAIN:[loss=0.686] VALID:[accuracy=0.549]\n",
      "[292 bat (0.47 epo)]: TRAIN:[loss=0.654] VALID:[accuracy=0.549]\n",
      "[293 bat (0.47 epo)]: TRAIN:[loss=0.659] VALID:[accuracy=0.545]\n",
      "[294 bat (0.47 epo)]: TRAIN:[loss=0.691] VALID:[accuracy=0.542]\n",
      "[295 bat (0.47 epo)]: TRAIN:[loss=0.694] VALID:[accuracy=0.542]\n",
      "[296 bat (0.48 epo)]: TRAIN:[loss=0.677] VALID:[accuracy=0.542]\n",
      "[297 bat (0.48 epo)]: TRAIN:[loss=0.698] VALID:[accuracy=0.538]\n",
      "[298 bat (0.48 epo)]: TRAIN:[loss=0.668] VALID:[accuracy=0.538]\n",
      "[299 bat (0.48 epo)]: TRAIN:[loss=0.662] VALID:[accuracy=0.538]\n",
      "[300 bat (0.48 epo)]: TRAIN:[loss=0.675] VALID:[accuracy=0.534]\n",
      "[301 bat (0.48 epo)]: TRAIN:[loss=0.639] VALID:[accuracy=0.542]\n",
      "[302 bat (0.49 epo)]: TRAIN:[loss=0.663] VALID:[accuracy=0.542]\n",
      "[303 bat (0.49 epo)]: TRAIN:[loss=0.672] VALID:[accuracy=0.538]\n",
      "[304 bat (0.49 epo)]: TRAIN:[loss=0.680] VALID:[accuracy=0.538]\n",
      "[305 bat (0.49 epo)]: TRAIN:[loss=0.666] VALID:[accuracy=0.542]\n",
      "[306 bat (0.49 epo)]: TRAIN:[loss=0.675] VALID:[accuracy=0.542]\n",
      "[307 bat (0.49 epo)]: TRAIN:[loss=0.700] VALID:[accuracy=0.552]\n",
      "[308 bat (0.49 epo)]: TRAIN:[loss=0.684] VALID:[accuracy=0.563]\n",
      "[309 bat (0.50 epo)]: TRAIN:[loss=0.710] VALID:[accuracy=0.560]\n",
      "[310 bat (0.50 epo)]: TRAIN:[loss=0.693] VALID:[accuracy=0.549]\n",
      "[311 bat (0.50 epo)]: TRAIN:[loss=0.711] VALID:[accuracy=0.545]\n",
      "[312 bat (0.50 epo)]: TRAIN:[loss=0.667] VALID:[accuracy=0.549]\n",
      "[313 bat (0.50 epo)]: TRAIN:[loss=0.669] VALID:[accuracy=0.542]\n",
      "[314 bat (0.50 epo)]: TRAIN:[loss=0.695] VALID:[accuracy=0.534]\n",
      "[315 bat (0.51 epo)]: TRAIN:[loss=0.676] VALID:[accuracy=0.527]\n",
      "[316 bat (0.51 epo)]: TRAIN:[loss=0.666] VALID:[accuracy=0.523]\n",
      "[317 bat (0.51 epo)]: TRAIN:[loss=0.701] VALID:[accuracy=0.523]\n",
      "[318 bat (0.51 epo)]: TRAIN:[loss=0.661] VALID:[accuracy=0.523]\n",
      "[319 bat (0.51 epo)]: TRAIN:[loss=0.674] VALID:[accuracy=0.523]\n",
      "[320 bat (0.51 epo)]: TRAIN:[loss=0.650] VALID:[accuracy=0.523]\n",
      "[321 bat (0.52 epo)]: TRAIN:[loss=0.685] VALID:[accuracy=0.527]\n",
      "[322 bat (0.52 epo)]: TRAIN:[loss=0.677] VALID:[accuracy=0.527]\n",
      "[323 bat (0.52 epo)]: TRAIN:[loss=0.674] VALID:[accuracy=0.527]\n",
      "[324 bat (0.52 epo)]: TRAIN:[loss=0.667] VALID:[accuracy=0.527]\n",
      "[325 bat (0.52 epo)]: TRAIN:[loss=0.678] VALID:[accuracy=0.527]\n",
      "[326 bat (0.52 epo)]: TRAIN:[loss=0.692] VALID:[accuracy=0.534]\n",
      "[327 bat (0.53 epo)]: TRAIN:[loss=0.648] VALID:[accuracy=0.542]\n",
      "[328 bat (0.53 epo)]: TRAIN:[loss=0.700] VALID:[accuracy=0.542]\n",
      "[329 bat (0.53 epo)]: TRAIN:[loss=0.679] VALID:[accuracy=0.534]\n",
      "[330 bat (0.53 epo)]: TRAIN:[loss=0.691] VALID:[accuracy=0.531]\n",
      "[331 bat (0.53 epo)]: TRAIN:[loss=0.690] VALID:[accuracy=0.542]\n",
      "[332 bat (0.53 epo)]: TRAIN:[loss=0.680] VALID:[accuracy=0.545]\n",
      "[333 bat (0.53 epo)]: TRAIN:[loss=0.666] VALID:[accuracy=0.542]\n",
      "[334 bat (0.54 epo)]: TRAIN:[loss=0.723] VALID:[accuracy=0.545]\n",
      "[335 bat (0.54 epo)]: TRAIN:[loss=0.679] VALID:[accuracy=0.545]\n",
      "[336 bat (0.54 epo)]: TRAIN:[loss=0.703] VALID:[accuracy=0.538]\n",
      "[337 bat (0.54 epo)]: TRAIN:[loss=0.663] VALID:[accuracy=0.542]\n",
      "[338 bat (0.54 epo)]: TRAIN:[loss=0.637] VALID:[accuracy=0.545]\n",
      "[339 bat (0.54 epo)]: TRAIN:[loss=0.686] VALID:[accuracy=0.542]\n",
      "[340 bat (0.55 epo)]: TRAIN:[loss=0.666] VALID:[accuracy=0.549]\n",
      "[341 bat (0.55 epo)]: TRAIN:[loss=0.665] VALID:[accuracy=0.549]\n",
      "[342 bat (0.55 epo)]: TRAIN:[loss=0.661] VALID:[accuracy=0.552]\n",
      "[343 bat (0.55 epo)]: TRAIN:[loss=0.685] VALID:[accuracy=0.552]\n",
      "[344 bat (0.55 epo)]: TRAIN:[loss=0.643] VALID:[accuracy=0.556]\n",
      "[345 bat (0.55 epo)]: TRAIN:[loss=0.679] VALID:[accuracy=0.552]\n",
      "[346 bat (0.56 epo)]: TRAIN:[loss=0.671] VALID:[accuracy=0.560]\n",
      "[347 bat (0.56 epo)]: TRAIN:[loss=0.652] VALID:[accuracy=0.563]\n",
      "[348 bat (0.56 epo)]: TRAIN:[loss=0.683] VALID:[accuracy=0.563]\n",
      "[349 bat (0.56 epo)]: TRAIN:[loss=0.692] VALID:[accuracy=0.552]\n",
      "[350 bat (0.56 epo)]: TRAIN:[loss=0.698] VALID:[accuracy=0.552]\n",
      "[351 bat (0.56 epo)]: TRAIN:[loss=0.673] VALID:[accuracy=0.545]\n",
      "[352 bat (0.57 epo)]: TRAIN:[loss=0.679] VALID:[accuracy=0.538]\n",
      "[353 bat (0.57 epo)]: TRAIN:[loss=0.683] VALID:[accuracy=0.560]\n",
      "[354 bat (0.57 epo)]: TRAIN:[loss=0.636] VALID:[accuracy=0.567]\n",
      "[355 bat (0.57 epo)]: TRAIN:[loss=0.659] VALID:[accuracy=0.570]\n",
      "[356 bat (0.57 epo)]: TRAIN:[loss=0.700] VALID:[accuracy=0.567]\n",
      "[357 bat (0.57 epo)]: TRAIN:[loss=0.672] VALID:[accuracy=0.570]\n",
      "[358 bat (0.58 epo)]: TRAIN:[loss=0.666] VALID:[accuracy=0.578]\n",
      "[359 bat (0.58 epo)]: TRAIN:[loss=0.663] VALID:[accuracy=0.574]\n",
      "[360 bat (0.58 epo)]: TRAIN:[loss=0.634] VALID:[accuracy=0.574]\n",
      "[361 bat (0.58 epo)]: TRAIN:[loss=0.675] VALID:[accuracy=0.567]\n",
      "[362 bat (0.58 epo)]: TRAIN:[loss=0.664] VALID:[accuracy=0.567]\n",
      "[363 bat (0.58 epo)]: TRAIN:[loss=0.660] VALID:[accuracy=0.570]\n",
      "[364 bat (0.58 epo)]: TRAIN:[loss=0.712] VALID:[accuracy=0.567]\n",
      "[365 bat (0.59 epo)]: TRAIN:[loss=0.647] VALID:[accuracy=0.560]\n",
      "[366 bat (0.59 epo)]: TRAIN:[loss=0.686] VALID:[accuracy=0.560]\n",
      "[367 bat (0.59 epo)]: TRAIN:[loss=0.637] VALID:[accuracy=0.563]\n",
      "[368 bat (0.59 epo)]: TRAIN:[loss=0.702] VALID:[accuracy=0.563]\n",
      "[369 bat (0.59 epo)]: TRAIN:[loss=0.680] VALID:[accuracy=0.556]\n",
      "[370 bat (0.59 epo)]: TRAIN:[loss=0.730] VALID:[accuracy=0.563]\n",
      "[371 bat (0.60 epo)]: TRAIN:[loss=0.674] VALID:[accuracy=0.552]\n",
      "[372 bat (0.60 epo)]: TRAIN:[loss=0.684] VALID:[accuracy=0.574]\n",
      "[373 bat (0.60 epo)]: TRAIN:[loss=0.665] VALID:[accuracy=0.560]\n",
      "[374 bat (0.60 epo)]: TRAIN:[loss=0.647] VALID:[accuracy=0.538]\n",
      "[375 bat (0.60 epo)]: TRAIN:[loss=0.706] VALID:[accuracy=0.542]\n",
      "[376 bat (0.60 epo)]: TRAIN:[loss=0.671] VALID:[accuracy=0.534]\n",
      "[377 bat (0.61 epo)]: TRAIN:[loss=0.688] VALID:[accuracy=0.538]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[378 bat (0.61 epo)]: TRAIN:[loss=0.668] VALID:[accuracy=0.538]\n",
      "[379 bat (0.61 epo)]: TRAIN:[loss=0.675] VALID:[accuracy=0.545]\n",
      "[380 bat (0.61 epo)]: TRAIN:[loss=0.665] VALID:[accuracy=0.534]\n",
      "[381 bat (0.61 epo)]: TRAIN:[loss=0.690] VALID:[accuracy=0.552]\n",
      "[382 bat (0.61 epo)]: TRAIN:[loss=0.691] VALID:[accuracy=0.552]\n",
      "[383 bat (0.62 epo)]: TRAIN:[loss=0.669] VALID:[accuracy=0.556]\n",
      "[384 bat (0.62 epo)]: TRAIN:[loss=0.672] VALID:[accuracy=0.552]\n",
      "[385 bat (0.62 epo)]: TRAIN:[loss=0.681] VALID:[accuracy=0.560]\n",
      "[386 bat (0.62 epo)]: TRAIN:[loss=0.678] VALID:[accuracy=0.560]\n",
      "[387 bat (0.62 epo)]: TRAIN:[loss=0.669] VALID:[accuracy=0.549]\n",
      "[388 bat (0.62 epo)]: TRAIN:[loss=0.674] VALID:[accuracy=0.542]\n",
      "[389 bat (0.62 epo)]: TRAIN:[loss=0.692] VALID:[accuracy=0.538]\n",
      "[390 bat (0.63 epo)]: TRAIN:[loss=0.701] VALID:[accuracy=0.534]\n",
      "Restoring best model from iteration 150 with score 0.596\n",
      "Finished Training\n",
      "Accuracy: 0.596\n",
      "        y=1    y=2   \n",
      " l=1    31     100   \n",
      " l=2    12     134   \n"
     ]
    }
   ],
   "source": [
    "end_model.train_model(\n",
    "    dataset[\"train\"].get_dataloader(batch_size=32),\n",
    "    valid_data=dataset[\"dev\"].get_dataloader(batch_size=32),\n",
    "#     dataloaders[\"train\"],\n",
    "#     valid_data=dataloaders[\"dev\"],\n",
    "    lr=5e-5,\n",
    "    l2=0,\n",
    "    n_epochs=3,\n",
    "#     checkpoint_metric=\"model/train/loss\",\n",
    "    checkpoint_metric=\"valid/accuracy\",\n",
    "    log_unit=\"batches\",\n",
    "    checkpoint_metric_mode=\"max\",\n",
    "    verbose=True,\n",
    "    progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.596\n",
      "Precision: 0.721\n",
      "Recall: 0.237\n",
      "F1: 0.356\n",
      "        y=1    y=2   \n",
      " l=1    31     100   \n",
      " l=2    12     134   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5956678700361011,\n",
       " 0.7209302325581395,\n",
       " 0.2366412213740458,\n",
       " 0.3563218390804598]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test end model\n",
    "end_model.score(dataset[\"dev\"].get_dataloader(batch_size=10), metric=[\"accuracy\", \"precision\", \"recall\", \"f1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
