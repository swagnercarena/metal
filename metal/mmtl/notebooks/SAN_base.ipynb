{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import metal\n",
    "import os\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "from dataset import QQPDataset, RTEDataset, WNLIDataset, MNLIDataset, MRPCDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177c00a50389442eb0571fad456aa8b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2490), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_ds = RTEDataset(split='train', bert_model='bert-base-uncased', max_len=128)\n",
    "train_dl, dev_dl = train_ds.get_dataloader(split_prop=0.8, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[  101,  5170,  6384,  ...,     0,     0,     0],\n",
      "        [  101,  1996,  2343,  ...,     0,     0,     0],\n",
      "        [  101,  2048, 28171,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  6972,  1048,  ...,     0,     0,     0],\n",
      "        [  101,  3174,  1011,  ...,     0,     0,     0],\n",
      "        [  101,  1996, 10863,  ...,     0,     0,     0]]), tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]))\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dl:\n",
    "    print(x)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# bert_model = 'bert-base-uncased'\n",
    "\n",
    "# class BertEncoder(nn.Module):\n",
    "#     def __init__(self, dropout=0.1):\n",
    "#         super(BertEncoder, self).__init__()\n",
    "#         self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, data):\n",
    "#         tokens, segments, mask = data\n",
    "#         _, hidden_layer = self.bert_model(tokens, segments, mask, output_all_encoded_layers=False)\n",
    "#         hidden_layer = self.dropout(hidden_layer)\n",
    "\n",
    "#         return hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "bert_model = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, bert_model='bert-base-uncased', dropout=0.1, cache_dir=\".\"):\n",
    "        super(BertEncoder, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained(bert_model, cache_dir=cache_dir)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, data):\n",
    "        tokens, segments, mask = data\n",
    "        output_layer, hidden_layer = self.bert_model(tokens, segments, mask, output_all_encoded_layers=False)\n",
    "        output_layer = self.dropout(output_layer)\n",
    "        hidden_layer = self.dropout(hidden_layer)\n",
    "        return output_layer, hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearLayer, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSelfAttn(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LinearSelfAttn, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "#         self.softmax = nn.Softmax(1)\n",
    "\n",
    "    def forward(self, x, x_mask):\n",
    "        scores = self.linear(x).view(x.size(0), x.size(1))\n",
    "        scores.data.masked_fill_(x_mask.data, -float('inf'))\n",
    "        alpha = torch.softmax(scores, 1)\n",
    "        return alpha.unsqueeze(1).bmm(x).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilinearSelfAttn(nn.Module):\n",
    "    def __init__(self, x_size, y_size):\n",
    "        super(BilinearSelfAttn, self).__init__()\n",
    "        self.linear = nn.Linear(y_size, x_size)\n",
    "#         self.softmax = nn.Softmax(1)\n",
    "\n",
    "    def forward(self, x, y, x_mask):\n",
    "        Wy = self.linear(y)\n",
    "        xWy = x.bmm(Wy.unsqueeze(2)).squeeze(2)\n",
    "        xWy.data.masked_fill_(x_mask.data, -float('inf'))\n",
    "        beta = torch.softmax(xWy, 1)\n",
    "        return beta.unsqueeze(1).bmm(x).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SAN(nn.Module):\n",
    "#     def __init__(self, emb_size=100, hidden_size=100, num_classes=2, k=5):\n",
    "#         super(SAN, self).__init__()\n",
    "#         self.bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "#         self.sent1_attn = LinearSelfAttn(input_size=emb_size)\n",
    "#         self.sent2_attn = BilinearSelfAttn(emb_size, emb_size)\n",
    "#         self.final_linear = nn.Linear(emb_size * 4, num_classes)\n",
    "#         self.rnn = rnn = nn.GRU(emb_size, hidden_size, 1, batch_first=True)\n",
    "#         self.k = k\n",
    "#         self.num_classes = num_classes\n",
    "#         self.softmax = nn.Softmax(1)\n",
    "        \n",
    "#         for param in self.bert_model.parameters():\n",
    "#             param.requires_grad = False\n",
    "\n",
    "#     def forward(self, X):\n",
    "# #         print(\"!\")\n",
    "#         sent1, sent1_mask, sent2, sent2_mask = X\n",
    "#         batch_size = sent1.size(0)\n",
    "#         sent1, _ = self.bert_model(\n",
    "#             sent1, sent1_mask, 1 - sent1_mask, output_all_encoded_layers=False\n",
    "#         )\n",
    "#         sent2, _ = self.bert_model(\n",
    "#             sent2, sent2_mask, 1 - sent2_mask, output_all_encoded_layers=False\n",
    "#         )\n",
    "#         res = sent1.new_zeros((batch_size, self.num_classes))\n",
    "# #         res = torch.zeros((batch_size, self.num_classes))\n",
    "#         sk = self.sent1_attn(sent1, sent1_mask.byte())\n",
    "\n",
    "#         for i in range(self.k):\n",
    "#             xk = self.sent2_attn(sent2, sk, sent2_mask.byte())\n",
    "#             _, sk = self.rnn(xk.unsqueeze(1), sk.unsqueeze(0))\n",
    "#             sk = sk.squeeze(0)\n",
    "\n",
    "#             f = self.softmax(\n",
    "#                 self.final_linear(torch.cat((sk, xk, torch.abs(sk - xk), sk * xk), 1))\n",
    "#             )\n",
    "#             res += f\n",
    "\n",
    "#         return res / self.k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_matrix, seg_matrix, mask_matrix\n",
    "\n",
    "class SAN(nn.Module):\n",
    "    def __init__(self, bert_model=BertEncoder(), emb_size=100, hidden_size=100, num_classes=2, k=5):\n",
    "        super(SAN, self).__init__()\n",
    "        self.bert_model = bert_model\n",
    "        self.sent1_attn = LinearSelfAttn(input_size=emb_size)\n",
    "        self.sent2_attn = BilinearSelfAttn(emb_size, emb_size)\n",
    "        self.final_linear = nn.Linear(emb_size * 4, num_classes)\n",
    "        self.rnn = rnn = nn.GRU(emb_size, hidden_size, 1, batch_first=True)\n",
    "        self.k = k\n",
    "        self.num_classes = num_classes\n",
    "#         self.softmax = nn.Softmax(1)\n",
    "        \n",
    "#         for param in self.bert_model.parameters():\n",
    "#             param.requires_grad = False\n",
    "\n",
    "    def forward(self, X):\n",
    "        idx_matrix, seg_matrix, mask_matrix = X\n",
    "#         sent1, sent1_mask, sent2, sent2_mask = X\n",
    "\n",
    "        batch_size = idx_matrix.size(0)\n",
    "        \n",
    "        output_layer, _ = self.bert_model.forward(X)\n",
    "       \n",
    "        res = output_layer.new_zeros((batch_size, self.num_classes))\n",
    "        \n",
    "        sk = self.sent1_attn(output_layer, (1 - mask_matrix + seg_matrix).byte())\n",
    "        \n",
    "#         sk = self.sent1_attn(sent1, sent1_mask.byte())\n",
    "\n",
    "        for i in range(self.k):\n",
    "            \n",
    "            xk = self.sent2_attn(output_layer, sk, (1 - seg_matrix).byte())\n",
    "        \n",
    "#             xk = self.sent2_attn(sent2, sk, sent2_mask.byte())\n",
    "            _, sk = self.rnn(xk.unsqueeze(1), sk.unsqueeze(0))\n",
    "            sk = sk.squeeze(0)\n",
    "\n",
    "            f = torch.softmax(\n",
    "                self.final_linear(torch.cat((sk, xk, torch.abs(sk - xk), sk * xk), 1)), 1\n",
    "            )\n",
    "            res += f\n",
    "\n",
    "        return res / self.k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "san = SAN(emb_size = 768, hidden_size = 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task = Task(task_name, dataloaders, BertEncoder(), task_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Network architecture:\n",
      "SAN(\n",
      "  (bert_model): BertEncoder(\n",
      "    (bert_model): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): BertLayerNorm()\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1)\n",
      "  )\n",
      "  (sent1_attn): LinearSelfAttn(\n",
      "    (linear): Linear(in_features=768, out_features=1, bias=True)\n",
      "  )\n",
      "  (sent2_attn): BilinearSelfAttn(\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (final_linear): Linear(in_features=3072, out_features=2, bias=True)\n",
      "  (rnn): GRU(768, 768, batch_first=True)\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from metal.end_model import EndModel\n",
    "end_model = EndModel(\n",
    "    [2], input_module=san, seed=123, device=\"cuda\", skip_head=True, input_relu=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU...\n",
      "[1 bat (0.00 epo)]: TRAIN:[loss=0.703] VALID:[accuracy=0.518]\n",
      "Saving model at iteration 1 with best (max) score 0.518\n",
      "[2 bat (0.00 epo)]: TRAIN:[loss=0.666] VALID:[accuracy=0.520]\n",
      "Saving model at iteration 2 with best (max) score 0.520\n",
      "[3 bat (0.00 epo)]: TRAIN:[loss=0.711] VALID:[accuracy=0.510]\n",
      "[4 bat (0.00 epo)]: TRAIN:[loss=0.723] VALID:[accuracy=0.490]\n",
      "[5 bat (0.01 epo)]: TRAIN:[loss=0.682] VALID:[accuracy=0.474]\n",
      "[6 bat (0.01 epo)]: TRAIN:[loss=0.683] VALID:[accuracy=0.472]\n",
      "[7 bat (0.01 epo)]: TRAIN:[loss=0.659] VALID:[accuracy=0.480]\n",
      "[8 bat (0.01 epo)]: TRAIN:[loss=0.707] VALID:[accuracy=0.470]\n",
      "[9 bat (0.01 epo)]: TRAIN:[loss=0.707] VALID:[accuracy=0.472]\n",
      "[10 bat (0.01 epo)]: TRAIN:[loss=0.646] VALID:[accuracy=0.466]\n",
      "[11 bat (0.01 epo)]: TRAIN:[loss=0.701] VALID:[accuracy=0.466]\n",
      "[12 bat (0.01 epo)]: TRAIN:[loss=0.633] VALID:[accuracy=0.468]\n",
      "[13 bat (0.02 epo)]: TRAIN:[loss=0.711] VALID:[accuracy=0.472]\n",
      "[14 bat (0.02 epo)]: TRAIN:[loss=0.744] VALID:[accuracy=0.474]\n",
      "[15 bat (0.02 epo)]: TRAIN:[loss=0.670] VALID:[accuracy=0.484]\n",
      "[16 bat (0.02 epo)]: TRAIN:[loss=0.725] VALID:[accuracy=0.500]\n",
      "[17 bat (0.02 epo)]: TRAIN:[loss=0.645] VALID:[accuracy=0.514]\n",
      "[18 bat (0.02 epo)]: TRAIN:[loss=0.679] VALID:[accuracy=0.520]\n",
      "[19 bat (0.02 epo)]: TRAIN:[loss=0.714] VALID:[accuracy=0.526]\n",
      "Saving model at iteration 19 with best (max) score 0.526\n",
      "[20 bat (0.02 epo)]: TRAIN:[loss=0.692] VALID:[accuracy=0.542]\n",
      "Saving model at iteration 20 with best (max) score 0.542\n",
      "[21 bat (0.03 epo)]: TRAIN:[loss=0.703] VALID:[accuracy=0.544]\n",
      "Saving model at iteration 21 with best (max) score 0.544\n",
      "[22 bat (0.03 epo)]: TRAIN:[loss=0.702] VALID:[accuracy=0.546]\n",
      "Saving model at iteration 22 with best (max) score 0.546\n",
      "[23 bat (0.03 epo)]: TRAIN:[loss=0.670] VALID:[accuracy=0.530]\n",
      "[24 bat (0.03 epo)]: TRAIN:[loss=0.682] VALID:[accuracy=0.526]\n",
      "[25 bat (0.03 epo)]: TRAIN:[loss=0.695] VALID:[accuracy=0.532]\n",
      "[26 bat (0.03 epo)]: TRAIN:[loss=0.719] VALID:[accuracy=0.524]\n",
      "[27 bat (0.03 epo)]: TRAIN:[loss=0.706] VALID:[accuracy=0.552]\n",
      "Saving model at iteration 27 with best (max) score 0.552\n",
      "[28 bat (0.03 epo)]: TRAIN:[loss=0.677] VALID:[accuracy=0.558]\n",
      "Saving model at iteration 28 with best (max) score 0.558\n",
      "[29 bat (0.03 epo)]: TRAIN:[loss=0.677] VALID:[accuracy=0.556]\n",
      "[30 bat (0.04 epo)]: TRAIN:[loss=0.697] VALID:[accuracy=0.556]\n",
      "[31 bat (0.04 epo)]: TRAIN:[loss=0.715] VALID:[accuracy=0.550]\n",
      "[32 bat (0.04 epo)]: TRAIN:[loss=0.677] VALID:[accuracy=0.560]\n",
      "Saving model at iteration 32 with best (max) score 0.560\n",
      "[33 bat (0.04 epo)]: TRAIN:[loss=0.722] VALID:[accuracy=0.534]\n",
      "[34 bat (0.04 epo)]: TRAIN:[loss=0.663] VALID:[accuracy=0.550]\n",
      "[35 bat (0.04 epo)]: TRAIN:[loss=0.666] VALID:[accuracy=0.546]\n",
      "[36 bat (0.04 epo)]: TRAIN:[loss=0.757] VALID:[accuracy=0.550]\n",
      "[37 bat (0.04 epo)]: TRAIN:[loss=0.671] VALID:[accuracy=0.542]\n",
      "[38 bat (0.05 epo)]: TRAIN:[loss=0.649] VALID:[accuracy=0.574]\n",
      "Saving model at iteration 38 with best (max) score 0.574\n",
      "[39 bat (0.05 epo)]: TRAIN:[loss=0.647] VALID:[accuracy=0.580]\n",
      "Saving model at iteration 39 with best (max) score 0.580\n",
      "[40 bat (0.05 epo)]: TRAIN:[loss=0.650] VALID:[accuracy=0.572]\n",
      "[41 bat (0.05 epo)]: TRAIN:[loss=0.675] VALID:[accuracy=0.564]\n",
      "[42 bat (0.05 epo)]: TRAIN:[loss=0.677] VALID:[accuracy=0.582]\n",
      "Saving model at iteration 42 with best (max) score 0.582\n",
      "[43 bat (0.05 epo)]: TRAIN:[loss=0.690] VALID:[accuracy=0.570]\n",
      "[44 bat (0.05 epo)]: TRAIN:[loss=0.714] VALID:[accuracy=0.538]\n",
      "[45 bat (0.05 epo)]: TRAIN:[loss=0.729] VALID:[accuracy=0.540]\n",
      "[46 bat (0.06 epo)]: TRAIN:[loss=0.650] VALID:[accuracy=0.506]\n",
      "[47 bat (0.06 epo)]: TRAIN:[loss=0.709] VALID:[accuracy=0.508]\n",
      "[48 bat (0.06 epo)]: TRAIN:[loss=0.776] VALID:[accuracy=0.506]\n",
      "[49 bat (0.06 epo)]: TRAIN:[loss=0.676] VALID:[accuracy=0.512]\n",
      "[50 bat (0.06 epo)]: TRAIN:[loss=0.631] VALID:[accuracy=0.518]\n",
      "[51 bat (0.06 epo)]: TRAIN:[loss=0.605] VALID:[accuracy=0.524]\n",
      "[52 bat (0.06 epo)]: TRAIN:[loss=0.697] VALID:[accuracy=0.526]\n",
      "[53 bat (0.06 epo)]: TRAIN:[loss=0.677] VALID:[accuracy=0.540]\n",
      "[54 bat (0.07 epo)]: TRAIN:[loss=0.610] VALID:[accuracy=0.568]\n",
      "[55 bat (0.07 epo)]: TRAIN:[loss=0.645] VALID:[accuracy=0.570]\n",
      "[56 bat (0.07 epo)]: TRAIN:[loss=0.613] VALID:[accuracy=0.568]\n",
      "[57 bat (0.07 epo)]: TRAIN:[loss=0.677] VALID:[accuracy=0.580]\n",
      "[58 bat (0.07 epo)]: TRAIN:[loss=0.668] VALID:[accuracy=0.610]\n",
      "Saving model at iteration 58 with best (max) score 0.610\n",
      "[59 bat (0.07 epo)]: TRAIN:[loss=0.705] VALID:[accuracy=0.592]\n",
      "[60 bat (0.07 epo)]: TRAIN:[loss=0.651] VALID:[accuracy=0.608]\n",
      "[61 bat (0.07 epo)]: TRAIN:[loss=0.667] VALID:[accuracy=0.604]\n",
      "[62 bat (0.07 epo)]: TRAIN:[loss=0.634] VALID:[accuracy=0.608]\n",
      "[63 bat (0.08 epo)]: TRAIN:[loss=0.698] VALID:[accuracy=0.612]\n",
      "Saving model at iteration 63 with best (max) score 0.612\n",
      "[64 bat (0.08 epo)]: TRAIN:[loss=0.625] VALID:[accuracy=0.608]\n",
      "[65 bat (0.08 epo)]: TRAIN:[loss=0.597] VALID:[accuracy=0.610]\n",
      "[66 bat (0.08 epo)]: TRAIN:[loss=0.603] VALID:[accuracy=0.582]\n",
      "[67 bat (0.08 epo)]: TRAIN:[loss=0.628] VALID:[accuracy=0.568]\n",
      "[68 bat (0.08 epo)]: TRAIN:[loss=0.729] VALID:[accuracy=0.584]\n",
      "[69 bat (0.08 epo)]: TRAIN:[loss=0.651] VALID:[accuracy=0.590]\n",
      "[70 bat (0.08 epo)]: TRAIN:[loss=0.623] VALID:[accuracy=0.598]\n",
      "[71 bat (0.09 epo)]: TRAIN:[loss=0.612] VALID:[accuracy=0.612]\n",
      "[72 bat (0.09 epo)]: TRAIN:[loss=0.715] VALID:[accuracy=0.637]\n",
      "Saving model at iteration 72 with best (max) score 0.637\n",
      "[73 bat (0.09 epo)]: TRAIN:[loss=0.654] VALID:[accuracy=0.631]\n",
      "[74 bat (0.09 epo)]: TRAIN:[loss=0.607] VALID:[accuracy=0.637]\n",
      "[75 bat (0.09 epo)]: TRAIN:[loss=0.722] VALID:[accuracy=0.647]\n",
      "Saving model at iteration 75 with best (max) score 0.647\n",
      "[76 bat (0.09 epo)]: TRAIN:[loss=0.626] VALID:[accuracy=0.631]\n",
      "[77 bat (0.09 epo)]: TRAIN:[loss=0.753] VALID:[accuracy=0.627]\n",
      "[78 bat (0.09 epo)]: TRAIN:[loss=0.692] VALID:[accuracy=0.624]\n",
      "[79 bat (0.10 epo)]: TRAIN:[loss=0.632] VALID:[accuracy=0.620]\n",
      "[80 bat (0.10 epo)]: TRAIN:[loss=0.800] VALID:[accuracy=0.641]\n",
      "[81 bat (0.10 epo)]: TRAIN:[loss=0.684] VALID:[accuracy=0.657]\n",
      "Saving model at iteration 81 with best (max) score 0.657\n",
      "[82 bat (0.10 epo)]: TRAIN:[loss=0.621] VALID:[accuracy=0.657]\n",
      "[83 bat (0.10 epo)]: TRAIN:[loss=0.653] VALID:[accuracy=0.622]\n",
      "[84 bat (0.10 epo)]: TRAIN:[loss=0.762] VALID:[accuracy=0.618]\n",
      "[85 bat (0.10 epo)]: TRAIN:[loss=0.559] VALID:[accuracy=0.606]\n",
      "[86 bat (0.10 epo)]: TRAIN:[loss=0.573] VALID:[accuracy=0.629]\n",
      "[87 bat (0.10 epo)]: TRAIN:[loss=0.616] VALID:[accuracy=0.629]\n",
      "[88 bat (0.11 epo)]: TRAIN:[loss=0.637] VALID:[accuracy=0.637]\n",
      "[89 bat (0.11 epo)]: TRAIN:[loss=0.608] VALID:[accuracy=0.641]\n",
      "[90 bat (0.11 epo)]: TRAIN:[loss=0.625] VALID:[accuracy=0.643]\n",
      "[91 bat (0.11 epo)]: TRAIN:[loss=0.619] VALID:[accuracy=0.647]\n",
      "[92 bat (0.11 epo)]: TRAIN:[loss=0.540] VALID:[accuracy=0.645]\n",
      "[93 bat (0.11 epo)]: TRAIN:[loss=0.612] VALID:[accuracy=0.639]\n",
      "[94 bat (0.11 epo)]: TRAIN:[loss=0.691] VALID:[accuracy=0.645]\n",
      "[95 bat (0.11 epo)]: TRAIN:[loss=0.701] VALID:[accuracy=0.649]\n",
      "[96 bat (0.12 epo)]: TRAIN:[loss=0.754] VALID:[accuracy=0.639]\n",
      "[97 bat (0.12 epo)]: TRAIN:[loss=0.504] VALID:[accuracy=0.635]\n",
      "[98 bat (0.12 epo)]: TRAIN:[loss=0.649] VALID:[accuracy=0.641]\n",
      "[99 bat (0.12 epo)]: TRAIN:[loss=0.775] VALID:[accuracy=0.612]\n",
      "[100 bat (0.12 epo)]: TRAIN:[loss=0.662] VALID:[accuracy=0.584]\n",
      "[101 bat (0.12 epo)]: TRAIN:[loss=0.614] VALID:[accuracy=0.582]\n",
      "[102 bat (0.12 epo)]: TRAIN:[loss=0.621] VALID:[accuracy=0.602]\n",
      "[103 bat (0.12 epo)]: TRAIN:[loss=0.657] VALID:[accuracy=0.639]\n",
      "[104 bat (0.13 epo)]: TRAIN:[loss=0.616] VALID:[accuracy=0.641]\n",
      "[105 bat (0.13 epo)]: TRAIN:[loss=0.644] VALID:[accuracy=0.641]\n",
      "[106 bat (0.13 epo)]: TRAIN:[loss=0.679] VALID:[accuracy=0.637]\n",
      "[107 bat (0.13 epo)]: TRAIN:[loss=0.587] VALID:[accuracy=0.631]\n",
      "[108 bat (0.13 epo)]: TRAIN:[loss=0.640] VALID:[accuracy=0.631]\n",
      "[109 bat (0.13 epo)]: TRAIN:[loss=0.679] VALID:[accuracy=0.651]\n",
      "[110 bat (0.13 epo)]: TRAIN:[loss=0.614] VALID:[accuracy=0.667]\n",
      "Saving model at iteration 110 with best (max) score 0.667\n",
      "[111 bat (0.13 epo)]: TRAIN:[loss=0.539] VALID:[accuracy=0.657]\n",
      "[112 bat (0.13 epo)]: TRAIN:[loss=0.683] VALID:[accuracy=0.645]\n",
      "[113 bat (0.14 epo)]: TRAIN:[loss=0.566] VALID:[accuracy=0.633]\n",
      "[114 bat (0.14 epo)]: TRAIN:[loss=0.615] VALID:[accuracy=0.622]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[115 bat (0.14 epo)]: TRAIN:[loss=0.570] VALID:[accuracy=0.633]\n",
      "[116 bat (0.14 epo)]: TRAIN:[loss=0.648] VALID:[accuracy=0.635]\n",
      "[117 bat (0.14 epo)]: TRAIN:[loss=0.538] VALID:[accuracy=0.673]\n",
      "Saving model at iteration 117 with best (max) score 0.673\n",
      "[118 bat (0.14 epo)]: TRAIN:[loss=0.708] VALID:[accuracy=0.643]\n",
      "[119 bat (0.14 epo)]: TRAIN:[loss=0.507] VALID:[accuracy=0.645]\n",
      "[120 bat (0.14 epo)]: TRAIN:[loss=0.570] VALID:[accuracy=0.653]\n",
      "[121 bat (0.15 epo)]: TRAIN:[loss=0.784] VALID:[accuracy=0.653]\n",
      "[122 bat (0.15 epo)]: TRAIN:[loss=0.645] VALID:[accuracy=0.647]\n",
      "[123 bat (0.15 epo)]: TRAIN:[loss=0.644] VALID:[accuracy=0.649]\n",
      "[124 bat (0.15 epo)]: TRAIN:[loss=0.626] VALID:[accuracy=0.639]\n",
      "[125 bat (0.15 epo)]: TRAIN:[loss=0.661] VALID:[accuracy=0.629]\n",
      "[126 bat (0.15 epo)]: TRAIN:[loss=0.552] VALID:[accuracy=0.629]\n",
      "[127 bat (0.15 epo)]: TRAIN:[loss=0.543] VALID:[accuracy=0.629]\n",
      "[128 bat (0.15 epo)]: TRAIN:[loss=0.466] VALID:[accuracy=0.629]\n",
      "[129 bat (0.16 epo)]: TRAIN:[loss=0.529] VALID:[accuracy=0.641]\n",
      "[130 bat (0.16 epo)]: TRAIN:[loss=0.531] VALID:[accuracy=0.663]\n",
      "[131 bat (0.16 epo)]: TRAIN:[loss=0.513] VALID:[accuracy=0.665]\n",
      "[132 bat (0.16 epo)]: TRAIN:[loss=0.522] VALID:[accuracy=0.655]\n",
      "[133 bat (0.16 epo)]: TRAIN:[loss=0.574] VALID:[accuracy=0.651]\n",
      "[134 bat (0.16 epo)]: TRAIN:[loss=0.447] VALID:[accuracy=0.651]\n",
      "[135 bat (0.16 epo)]: TRAIN:[loss=0.495] VALID:[accuracy=0.647]\n",
      "[136 bat (0.16 epo)]: TRAIN:[loss=0.505] VALID:[accuracy=0.659]\n",
      "[137 bat (0.17 epo)]: TRAIN:[loss=0.674] VALID:[accuracy=0.655]\n",
      "[138 bat (0.17 epo)]: TRAIN:[loss=0.560] VALID:[accuracy=0.663]\n",
      "[139 bat (0.17 epo)]: TRAIN:[loss=0.466] VALID:[accuracy=0.659]\n",
      "[140 bat (0.17 epo)]: TRAIN:[loss=0.460] VALID:[accuracy=0.659]\n",
      "[141 bat (0.17 epo)]: TRAIN:[loss=0.512] VALID:[accuracy=0.659]\n",
      "[142 bat (0.17 epo)]: TRAIN:[loss=0.522] VALID:[accuracy=0.659]\n",
      "[143 bat (0.17 epo)]: TRAIN:[loss=0.414] VALID:[accuracy=0.643]\n",
      "[144 bat (0.17 epo)]: TRAIN:[loss=0.458] VALID:[accuracy=0.643]\n",
      "[145 bat (0.17 epo)]: TRAIN:[loss=0.639] VALID:[accuracy=0.637]\n",
      "[146 bat (0.18 epo)]: TRAIN:[loss=0.502] VALID:[accuracy=0.635]\n",
      "[147 bat (0.18 epo)]: TRAIN:[loss=0.653] VALID:[accuracy=0.637]\n",
      "[148 bat (0.18 epo)]: TRAIN:[loss=0.546] VALID:[accuracy=0.643]\n",
      "[149 bat (0.18 epo)]: TRAIN:[loss=0.588] VALID:[accuracy=0.637]\n",
      "[150 bat (0.18 epo)]: TRAIN:[loss=0.461] VALID:[accuracy=0.637]\n",
      "[151 bat (0.18 epo)]: TRAIN:[loss=0.552] VALID:[accuracy=0.639]\n",
      "[152 bat (0.18 epo)]: TRAIN:[loss=0.737] VALID:[accuracy=0.629]\n",
      "[153 bat (0.18 epo)]: TRAIN:[loss=0.616] VALID:[accuracy=0.641]\n",
      "[154 bat (0.19 epo)]: TRAIN:[loss=0.398] VALID:[accuracy=0.643]\n",
      "[155 bat (0.19 epo)]: TRAIN:[loss=0.480] VALID:[accuracy=0.649]\n",
      "[156 bat (0.19 epo)]: TRAIN:[loss=0.742] VALID:[accuracy=0.647]\n",
      "[157 bat (0.19 epo)]: TRAIN:[loss=0.512] VALID:[accuracy=0.647]\n",
      "[158 bat (0.19 epo)]: TRAIN:[loss=0.469] VALID:[accuracy=0.641]\n",
      "[159 bat (0.19 epo)]: TRAIN:[loss=0.603] VALID:[accuracy=0.641]\n",
      "[160 bat (0.19 epo)]: TRAIN:[loss=0.572] VALID:[accuracy=0.651]\n",
      "[161 bat (0.19 epo)]: TRAIN:[loss=0.395] VALID:[accuracy=0.639]\n",
      "[162 bat (0.20 epo)]: TRAIN:[loss=0.493] VALID:[accuracy=0.639]\n",
      "[163 bat (0.20 epo)]: TRAIN:[loss=0.459] VALID:[accuracy=0.637]\n",
      "[164 bat (0.20 epo)]: TRAIN:[loss=0.382] VALID:[accuracy=0.637]\n",
      "[165 bat (0.20 epo)]: TRAIN:[loss=0.463] VALID:[accuracy=0.635]\n",
      "[166 bat (0.20 epo)]: TRAIN:[loss=0.825] VALID:[accuracy=0.639]\n",
      "[167 bat (0.20 epo)]: TRAIN:[loss=0.357] VALID:[accuracy=0.645]\n",
      "[168 bat (0.20 epo)]: TRAIN:[loss=0.510] VALID:[accuracy=0.645]\n",
      "[169 bat (0.20 epo)]: TRAIN:[loss=0.452] VALID:[accuracy=0.645]\n",
      "[170 bat (0.20 epo)]: TRAIN:[loss=0.578] VALID:[accuracy=0.653]\n",
      "[171 bat (0.21 epo)]: TRAIN:[loss=0.403] VALID:[accuracy=0.657]\n",
      "[172 bat (0.21 epo)]: TRAIN:[loss=0.669] VALID:[accuracy=0.647]\n",
      "[173 bat (0.21 epo)]: TRAIN:[loss=0.555] VALID:[accuracy=0.643]\n",
      "[174 bat (0.21 epo)]: TRAIN:[loss=0.517] VALID:[accuracy=0.647]\n",
      "[175 bat (0.21 epo)]: TRAIN:[loss=0.559] VALID:[accuracy=0.641]\n",
      "[176 bat (0.21 epo)]: TRAIN:[loss=0.538] VALID:[accuracy=0.651]\n",
      "[177 bat (0.21 epo)]: TRAIN:[loss=0.427] VALID:[accuracy=0.663]\n",
      "[178 bat (0.21 epo)]: TRAIN:[loss=0.508] VALID:[accuracy=0.657]\n",
      "[179 bat (0.22 epo)]: TRAIN:[loss=0.599] VALID:[accuracy=0.651]\n",
      "[180 bat (0.22 epo)]: TRAIN:[loss=0.564] VALID:[accuracy=0.653]\n",
      "[181 bat (0.22 epo)]: TRAIN:[loss=0.539] VALID:[accuracy=0.653]\n",
      "[182 bat (0.22 epo)]: TRAIN:[loss=0.561] VALID:[accuracy=0.661]\n",
      "[183 bat (0.22 epo)]: TRAIN:[loss=0.452] VALID:[accuracy=0.661]\n",
      "[184 bat (0.22 epo)]: TRAIN:[loss=0.404] VALID:[accuracy=0.653]\n",
      "[185 bat (0.22 epo)]: TRAIN:[loss=0.512] VALID:[accuracy=0.653]\n",
      "[186 bat (0.22 epo)]: TRAIN:[loss=0.557] VALID:[accuracy=0.633]\n",
      "[187 bat (0.23 epo)]: TRAIN:[loss=0.535] VALID:[accuracy=0.637]\n",
      "[188 bat (0.23 epo)]: TRAIN:[loss=0.577] VALID:[accuracy=0.647]\n",
      "[189 bat (0.23 epo)]: TRAIN:[loss=0.490] VALID:[accuracy=0.647]\n",
      "[190 bat (0.23 epo)]: TRAIN:[loss=0.426] VALID:[accuracy=0.671]\n",
      "[191 bat (0.23 epo)]: TRAIN:[loss=0.485] VALID:[accuracy=0.675]\n",
      "Saving model at iteration 191 with best (max) score 0.675\n",
      "[192 bat (0.23 epo)]: TRAIN:[loss=0.558] VALID:[accuracy=0.659]\n",
      "[193 bat (0.23 epo)]: TRAIN:[loss=0.571] VALID:[accuracy=0.629]\n",
      "[194 bat (0.23 epo)]: TRAIN:[loss=0.716] VALID:[accuracy=0.655]\n",
      "[195 bat (0.23 epo)]: TRAIN:[loss=0.564] VALID:[accuracy=0.661]\n",
      "[196 bat (0.24 epo)]: TRAIN:[loss=0.597] VALID:[accuracy=0.665]\n",
      "[197 bat (0.24 epo)]: TRAIN:[loss=0.650] VALID:[accuracy=0.665]\n",
      "[198 bat (0.24 epo)]: TRAIN:[loss=0.568] VALID:[accuracy=0.655]\n",
      "[199 bat (0.24 epo)]: TRAIN:[loss=0.449] VALID:[accuracy=0.661]\n",
      "[200 bat (0.24 epo)]: TRAIN:[loss=0.476] VALID:[accuracy=0.651]\n",
      "[201 bat (0.24 epo)]: TRAIN:[loss=0.472] VALID:[accuracy=0.649]\n",
      "[202 bat (0.24 epo)]: TRAIN:[loss=0.639] VALID:[accuracy=0.659]\n",
      "[203 bat (0.24 epo)]: TRAIN:[loss=0.404] VALID:[accuracy=0.669]\n",
      "[204 bat (0.25 epo)]: TRAIN:[loss=0.557] VALID:[accuracy=0.669]\n",
      "[205 bat (0.25 epo)]: TRAIN:[loss=0.500] VALID:[accuracy=0.671]\n",
      "[206 bat (0.25 epo)]: TRAIN:[loss=0.519] VALID:[accuracy=0.673]\n",
      "[207 bat (0.25 epo)]: TRAIN:[loss=0.538] VALID:[accuracy=0.667]\n",
      "[208 bat (0.25 epo)]: TRAIN:[loss=0.497] VALID:[accuracy=0.675]\n",
      "[209 bat (0.25 epo)]: TRAIN:[loss=0.704] VALID:[accuracy=0.677]\n",
      "Saving model at iteration 209 with best (max) score 0.677\n",
      "[210 bat (0.25 epo)]: TRAIN:[loss=0.670] VALID:[accuracy=0.679]\n",
      "Saving model at iteration 210 with best (max) score 0.679\n",
      "[211 bat (0.25 epo)]: TRAIN:[loss=0.356] VALID:[accuracy=0.675]\n",
      "[212 bat (0.26 epo)]: TRAIN:[loss=0.431] VALID:[accuracy=0.663]\n",
      "[213 bat (0.26 epo)]: TRAIN:[loss=0.471] VALID:[accuracy=0.637]\n",
      "[214 bat (0.26 epo)]: TRAIN:[loss=0.545] VALID:[accuracy=0.637]\n",
      "[215 bat (0.26 epo)]: TRAIN:[loss=0.738] VALID:[accuracy=0.641]\n",
      "[216 bat (0.26 epo)]: TRAIN:[loss=0.579] VALID:[accuracy=0.647]\n",
      "[217 bat (0.26 epo)]: TRAIN:[loss=0.686] VALID:[accuracy=0.645]\n",
      "[218 bat (0.26 epo)]: TRAIN:[loss=0.558] VALID:[accuracy=0.667]\n",
      "[219 bat (0.26 epo)]: TRAIN:[loss=0.558] VALID:[accuracy=0.677]\n",
      "[220 bat (0.27 epo)]: TRAIN:[loss=0.539] VALID:[accuracy=0.675]\n",
      "[221 bat (0.27 epo)]: TRAIN:[loss=0.527] VALID:[accuracy=0.651]\n",
      "[222 bat (0.27 epo)]: TRAIN:[loss=0.619] VALID:[accuracy=0.651]\n",
      "[223 bat (0.27 epo)]: TRAIN:[loss=0.769] VALID:[accuracy=0.649]\n",
      "[224 bat (0.27 epo)]: TRAIN:[loss=0.519] VALID:[accuracy=0.657]\n",
      "[225 bat (0.27 epo)]: TRAIN:[loss=0.677] VALID:[accuracy=0.669]\n",
      "[226 bat (0.27 epo)]: TRAIN:[loss=0.704] VALID:[accuracy=0.683]\n",
      "Saving model at iteration 226 with best (max) score 0.683\n",
      "[227 bat (0.27 epo)]: TRAIN:[loss=0.578] VALID:[accuracy=0.673]\n",
      "[228 bat (0.27 epo)]: TRAIN:[loss=0.494] VALID:[accuracy=0.653]\n",
      "[229 bat (0.28 epo)]: TRAIN:[loss=0.466] VALID:[accuracy=0.651]\n",
      "[230 bat (0.28 epo)]: TRAIN:[loss=0.618] VALID:[accuracy=0.651]\n",
      "[231 bat (0.28 epo)]: TRAIN:[loss=0.524] VALID:[accuracy=0.645]\n",
      "[232 bat (0.28 epo)]: TRAIN:[loss=0.537] VALID:[accuracy=0.643]\n",
      "[233 bat (0.28 epo)]: TRAIN:[loss=0.484] VALID:[accuracy=0.665]\n",
      "[234 bat (0.28 epo)]: TRAIN:[loss=0.645] VALID:[accuracy=0.663]\n",
      "[235 bat (0.28 epo)]: TRAIN:[loss=0.638] VALID:[accuracy=0.665]\n",
      "[236 bat (0.28 epo)]: TRAIN:[loss=0.522] VALID:[accuracy=0.669]\n",
      "[237 bat (0.29 epo)]: TRAIN:[loss=0.459] VALID:[accuracy=0.667]\n",
      "[238 bat (0.29 epo)]: TRAIN:[loss=0.603] VALID:[accuracy=0.663]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[239 bat (0.29 epo)]: TRAIN:[loss=0.705] VALID:[accuracy=0.657]\n",
      "[240 bat (0.29 epo)]: TRAIN:[loss=0.613] VALID:[accuracy=0.645]\n",
      "[241 bat (0.29 epo)]: TRAIN:[loss=0.420] VALID:[accuracy=0.645]\n",
      "[242 bat (0.29 epo)]: TRAIN:[loss=0.540] VALID:[accuracy=0.645]\n",
      "[243 bat (0.29 epo)]: TRAIN:[loss=0.596] VALID:[accuracy=0.651]\n",
      "[244 bat (0.29 epo)]: TRAIN:[loss=0.578] VALID:[accuracy=0.659]\n",
      "[245 bat (0.30 epo)]: TRAIN:[loss=0.579] VALID:[accuracy=0.657]\n",
      "[246 bat (0.30 epo)]: TRAIN:[loss=0.545] VALID:[accuracy=0.673]\n",
      "[247 bat (0.30 epo)]: TRAIN:[loss=0.576] VALID:[accuracy=0.667]\n",
      "[248 bat (0.30 epo)]: TRAIN:[loss=0.629] VALID:[accuracy=0.673]\n",
      "[249 bat (0.30 epo)]: TRAIN:[loss=0.665] VALID:[accuracy=0.675]\n",
      "[250 bat (0.30 epo)]: TRAIN:[loss=0.530] VALID:[accuracy=0.661]\n",
      "[251 bat (0.30 epo)]: TRAIN:[loss=0.470] VALID:[accuracy=0.667]\n",
      "[252 bat (0.30 epo)]: TRAIN:[loss=0.453] VALID:[accuracy=0.653]\n",
      "[253 bat (0.30 epo)]: TRAIN:[loss=0.563] VALID:[accuracy=0.639]\n",
      "[254 bat (0.31 epo)]: TRAIN:[loss=0.406] VALID:[accuracy=0.635]\n",
      "[255 bat (0.31 epo)]: TRAIN:[loss=0.547] VALID:[accuracy=0.633]\n",
      "[256 bat (0.31 epo)]: TRAIN:[loss=0.605] VALID:[accuracy=0.631]\n",
      "[257 bat (0.31 epo)]: TRAIN:[loss=0.448] VALID:[accuracy=0.631]\n",
      "[258 bat (0.31 epo)]: TRAIN:[loss=0.418] VALID:[accuracy=0.645]\n",
      "[259 bat (0.31 epo)]: TRAIN:[loss=0.626] VALID:[accuracy=0.659]\n",
      "[260 bat (0.31 epo)]: TRAIN:[loss=0.511] VALID:[accuracy=0.657]\n",
      "[261 bat (0.31 epo)]: TRAIN:[loss=0.577] VALID:[accuracy=0.667]\n",
      "[262 bat (0.32 epo)]: TRAIN:[loss=0.504] VALID:[accuracy=0.663]\n",
      "[263 bat (0.32 epo)]: TRAIN:[loss=0.485] VALID:[accuracy=0.661]\n",
      "[264 bat (0.32 epo)]: TRAIN:[loss=0.503] VALID:[accuracy=0.655]\n",
      "[265 bat (0.32 epo)]: TRAIN:[loss=0.464] VALID:[accuracy=0.635]\n",
      "[266 bat (0.32 epo)]: TRAIN:[loss=0.356] VALID:[accuracy=0.629]\n",
      "[267 bat (0.32 epo)]: TRAIN:[loss=0.482] VALID:[accuracy=0.618]\n",
      "[268 bat (0.32 epo)]: TRAIN:[loss=0.447] VALID:[accuracy=0.616]\n",
      "[269 bat (0.32 epo)]: TRAIN:[loss=0.505] VALID:[accuracy=0.624]\n",
      "[270 bat (0.33 epo)]: TRAIN:[loss=0.322] VALID:[accuracy=0.629]\n",
      "[271 bat (0.33 epo)]: TRAIN:[loss=0.461] VALID:[accuracy=0.635]\n",
      "[272 bat (0.33 epo)]: TRAIN:[loss=0.529] VALID:[accuracy=0.643]\n",
      "[273 bat (0.33 epo)]: TRAIN:[loss=0.411] VALID:[accuracy=0.669]\n",
      "[274 bat (0.33 epo)]: TRAIN:[loss=0.496] VALID:[accuracy=0.657]\n",
      "[275 bat (0.33 epo)]: TRAIN:[loss=0.360] VALID:[accuracy=0.655]\n",
      "[276 bat (0.33 epo)]: TRAIN:[loss=0.471] VALID:[accuracy=0.659]\n",
      "[277 bat (0.33 epo)]: TRAIN:[loss=0.480] VALID:[accuracy=0.671]\n",
      "[278 bat (0.33 epo)]: TRAIN:[loss=0.480] VALID:[accuracy=0.663]\n",
      "[279 bat (0.34 epo)]: TRAIN:[loss=0.643] VALID:[accuracy=0.669]\n",
      "[280 bat (0.34 epo)]: TRAIN:[loss=0.638] VALID:[accuracy=0.669]\n",
      "[281 bat (0.34 epo)]: TRAIN:[loss=0.592] VALID:[accuracy=0.663]\n",
      "[282 bat (0.34 epo)]: TRAIN:[loss=0.567] VALID:[accuracy=0.665]\n",
      "[283 bat (0.34 epo)]: TRAIN:[loss=0.517] VALID:[accuracy=0.657]\n",
      "[284 bat (0.34 epo)]: TRAIN:[loss=0.405] VALID:[accuracy=0.661]\n",
      "[285 bat (0.34 epo)]: TRAIN:[loss=0.468] VALID:[accuracy=0.667]\n",
      "[286 bat (0.34 epo)]: TRAIN:[loss=0.418] VALID:[accuracy=0.649]\n",
      "[287 bat (0.35 epo)]: TRAIN:[loss=0.401] VALID:[accuracy=0.649]\n",
      "[288 bat (0.35 epo)]: TRAIN:[loss=0.479] VALID:[accuracy=0.653]\n",
      "[289 bat (0.35 epo)]: TRAIN:[loss=0.403] VALID:[accuracy=0.647]\n",
      "[290 bat (0.35 epo)]: TRAIN:[loss=0.411] VALID:[accuracy=0.649]\n",
      "[291 bat (0.35 epo)]: TRAIN:[loss=0.359] VALID:[accuracy=0.655]\n",
      "[292 bat (0.35 epo)]: TRAIN:[loss=0.534] VALID:[accuracy=0.659]\n",
      "[293 bat (0.35 epo)]: TRAIN:[loss=0.511] VALID:[accuracy=0.673]\n",
      "[294 bat (0.35 epo)]: TRAIN:[loss=0.423] VALID:[accuracy=0.667]\n",
      "[295 bat (0.36 epo)]: TRAIN:[loss=0.553] VALID:[accuracy=0.671]\n",
      "[296 bat (0.36 epo)]: TRAIN:[loss=0.344] VALID:[accuracy=0.651]\n",
      "[297 bat (0.36 epo)]: TRAIN:[loss=0.409] VALID:[accuracy=0.653]\n",
      "[298 bat (0.36 epo)]: TRAIN:[loss=0.538] VALID:[accuracy=0.659]\n",
      "[299 bat (0.36 epo)]: TRAIN:[loss=0.533] VALID:[accuracy=0.659]\n",
      "[300 bat (0.36 epo)]: TRAIN:[loss=0.393] VALID:[accuracy=0.657]\n",
      "[301 bat (0.36 epo)]: TRAIN:[loss=0.481] VALID:[accuracy=0.661]\n",
      "[302 bat (0.36 epo)]: TRAIN:[loss=0.348] VALID:[accuracy=0.669]\n",
      "[303 bat (0.37 epo)]: TRAIN:[loss=0.430] VALID:[accuracy=0.661]\n",
      "[304 bat (0.37 epo)]: TRAIN:[loss=0.586] VALID:[accuracy=0.657]\n",
      "[305 bat (0.37 epo)]: TRAIN:[loss=0.420] VALID:[accuracy=0.649]\n",
      "[306 bat (0.37 epo)]: TRAIN:[loss=0.447] VALID:[accuracy=0.635]\n",
      "[307 bat (0.37 epo)]: TRAIN:[loss=0.444] VALID:[accuracy=0.629]\n",
      "[308 bat (0.37 epo)]: TRAIN:[loss=0.664] VALID:[accuracy=0.622]\n",
      "[309 bat (0.37 epo)]: TRAIN:[loss=0.447] VALID:[accuracy=0.618]\n",
      "[310 bat (0.37 epo)]: TRAIN:[loss=0.498] VALID:[accuracy=0.629]\n",
      "[311 bat (0.37 epo)]: TRAIN:[loss=0.483] VALID:[accuracy=0.631]\n",
      "[312 bat (0.38 epo)]: TRAIN:[loss=0.577] VALID:[accuracy=0.647]\n",
      "[313 bat (0.38 epo)]: TRAIN:[loss=0.394] VALID:[accuracy=0.661]\n",
      "[314 bat (0.38 epo)]: TRAIN:[loss=0.389] VALID:[accuracy=0.687]\n",
      "Saving model at iteration 314 with best (max) score 0.687\n",
      "[315 bat (0.38 epo)]: TRAIN:[loss=0.406] VALID:[accuracy=0.673]\n",
      "[316 bat (0.38 epo)]: TRAIN:[loss=0.378] VALID:[accuracy=0.665]\n",
      "[317 bat (0.38 epo)]: TRAIN:[loss=0.454] VALID:[accuracy=0.681]\n",
      "[318 bat (0.38 epo)]: TRAIN:[loss=0.534] VALID:[accuracy=0.675]\n",
      "[319 bat (0.38 epo)]: TRAIN:[loss=0.523] VALID:[accuracy=0.677]\n",
      "[320 bat (0.39 epo)]: TRAIN:[loss=0.417] VALID:[accuracy=0.687]\n",
      "[321 bat (0.39 epo)]: TRAIN:[loss=0.320] VALID:[accuracy=0.691]\n",
      "Saving model at iteration 321 with best (max) score 0.691\n",
      "[322 bat (0.39 epo)]: TRAIN:[loss=0.512] VALID:[accuracy=0.689]\n",
      "[323 bat (0.39 epo)]: TRAIN:[loss=0.620] VALID:[accuracy=0.685]\n",
      "[324 bat (0.39 epo)]: TRAIN:[loss=0.524] VALID:[accuracy=0.687]\n",
      "[325 bat (0.39 epo)]: TRAIN:[loss=0.454] VALID:[accuracy=0.689]\n",
      "[326 bat (0.39 epo)]: TRAIN:[loss=0.327] VALID:[accuracy=0.685]\n",
      "[327 bat (0.39 epo)]: TRAIN:[loss=0.398] VALID:[accuracy=0.687]\n",
      "[328 bat (0.40 epo)]: TRAIN:[loss=0.448] VALID:[accuracy=0.675]\n",
      "[329 bat (0.40 epo)]: TRAIN:[loss=0.408] VALID:[accuracy=0.663]\n",
      "[330 bat (0.40 epo)]: TRAIN:[loss=0.545] VALID:[accuracy=0.665]\n",
      "[331 bat (0.40 epo)]: TRAIN:[loss=0.438] VALID:[accuracy=0.671]\n",
      "[332 bat (0.40 epo)]: TRAIN:[loss=0.443] VALID:[accuracy=0.659]\n",
      "[333 bat (0.40 epo)]: TRAIN:[loss=0.437] VALID:[accuracy=0.653]\n",
      "[334 bat (0.40 epo)]: TRAIN:[loss=0.475] VALID:[accuracy=0.645]\n",
      "[335 bat (0.40 epo)]: TRAIN:[loss=0.734] VALID:[accuracy=0.645]\n",
      "[336 bat (0.40 epo)]: TRAIN:[loss=0.453] VALID:[accuracy=0.653]\n",
      "[337 bat (0.41 epo)]: TRAIN:[loss=0.395] VALID:[accuracy=0.661]\n",
      "[338 bat (0.41 epo)]: TRAIN:[loss=0.356] VALID:[accuracy=0.659]\n",
      "[339 bat (0.41 epo)]: TRAIN:[loss=0.496] VALID:[accuracy=0.663]\n",
      "[340 bat (0.41 epo)]: TRAIN:[loss=0.328] VALID:[accuracy=0.665]\n",
      "[341 bat (0.41 epo)]: TRAIN:[loss=0.421] VALID:[accuracy=0.675]\n",
      "[342 bat (0.41 epo)]: TRAIN:[loss=0.488] VALID:[accuracy=0.665]\n",
      "[343 bat (0.41 epo)]: TRAIN:[loss=0.319] VALID:[accuracy=0.659]\n",
      "[344 bat (0.41 epo)]: TRAIN:[loss=0.333] VALID:[accuracy=0.657]\n",
      "[345 bat (0.42 epo)]: TRAIN:[loss=0.375] VALID:[accuracy=0.643]\n",
      "[346 bat (0.42 epo)]: TRAIN:[loss=0.381] VALID:[accuracy=0.635]\n",
      "[347 bat (0.42 epo)]: TRAIN:[loss=0.565] VALID:[accuracy=0.647]\n",
      "[348 bat (0.42 epo)]: TRAIN:[loss=0.393] VALID:[accuracy=0.631]\n",
      "[349 bat (0.42 epo)]: TRAIN:[loss=0.479] VALID:[accuracy=0.618]\n",
      "[350 bat (0.42 epo)]: TRAIN:[loss=0.319] VALID:[accuracy=0.610]\n",
      "[351 bat (0.42 epo)]: TRAIN:[loss=0.624] VALID:[accuracy=0.608]\n",
      "[352 bat (0.42 epo)]: TRAIN:[loss=0.446] VALID:[accuracy=0.610]\n",
      "[353 bat (0.43 epo)]: TRAIN:[loss=0.733] VALID:[accuracy=0.612]\n",
      "[354 bat (0.43 epo)]: TRAIN:[loss=0.421] VALID:[accuracy=0.618]\n",
      "[355 bat (0.43 epo)]: TRAIN:[loss=0.456] VALID:[accuracy=0.629]\n",
      "[356 bat (0.43 epo)]: TRAIN:[loss=0.489] VALID:[accuracy=0.624]\n",
      "[357 bat (0.43 epo)]: TRAIN:[loss=0.480] VALID:[accuracy=0.616]\n",
      "[358 bat (0.43 epo)]: TRAIN:[loss=0.380] VALID:[accuracy=0.627]\n",
      "[359 bat (0.43 epo)]: TRAIN:[loss=0.499] VALID:[accuracy=0.643]\n",
      "[360 bat (0.43 epo)]: TRAIN:[loss=0.361] VALID:[accuracy=0.655]\n",
      "[361 bat (0.43 epo)]: TRAIN:[loss=0.379] VALID:[accuracy=0.657]\n",
      "[362 bat (0.44 epo)]: TRAIN:[loss=0.434] VALID:[accuracy=0.641]\n",
      "[363 bat (0.44 epo)]: TRAIN:[loss=0.380] VALID:[accuracy=0.612]\n",
      "[364 bat (0.44 epo)]: TRAIN:[loss=0.526] VALID:[accuracy=0.618]\n",
      "[365 bat (0.44 epo)]: TRAIN:[loss=0.563] VALID:[accuracy=0.594]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[366 bat (0.44 epo)]: TRAIN:[loss=0.449] VALID:[accuracy=0.590]\n",
      "[367 bat (0.44 epo)]: TRAIN:[loss=0.688] VALID:[accuracy=0.582]\n",
      "[368 bat (0.44 epo)]: TRAIN:[loss=0.582] VALID:[accuracy=0.582]\n",
      "[369 bat (0.44 epo)]: TRAIN:[loss=0.616] VALID:[accuracy=0.586]\n",
      "[370 bat (0.45 epo)]: TRAIN:[loss=0.571] VALID:[accuracy=0.584]\n",
      "[371 bat (0.45 epo)]: TRAIN:[loss=0.611] VALID:[accuracy=0.588]\n",
      "[372 bat (0.45 epo)]: TRAIN:[loss=0.538] VALID:[accuracy=0.608]\n",
      "[373 bat (0.45 epo)]: TRAIN:[loss=0.417] VALID:[accuracy=0.618]\n",
      "[374 bat (0.45 epo)]: TRAIN:[loss=0.692] VALID:[accuracy=0.641]\n",
      "[375 bat (0.45 epo)]: TRAIN:[loss=0.317] VALID:[accuracy=0.651]\n",
      "Restoring best model from iteration 321 with score 0.691\n",
      "Finished Training\n",
      "Accuracy: 0.691\n",
      "        y=1    y=2   \n",
      " l=1    194    112   \n",
      " l=2    42     150   \n"
     ]
    }
   ],
   "source": [
    "end_model.train_model(\n",
    "    train_dl,\n",
    "#     dataset[\"train\"].get_dataloader(batch_size=32),\n",
    "    valid_data=dev_dl,\n",
    "#     valid_data=dataset[\"dev\"].get_dataloader(batch_size=32),\n",
    "#     dataloaders[\"train\"],\n",
    "#     valid_data=dataloaders[\"dev\"],\n",
    "    lr=5e-5,\n",
    "    l2=0,\n",
    "    n_epochs=3,\n",
    "#     checkpoint_metric=\"model/train/loss\",\n",
    "    checkpoint_metric=\"valid/accuracy\",\n",
    "    log_unit=\"batches\",\n",
    "    checkpoint_metric_mode=\"max\",\n",
    "    verbose=True,\n",
    "    progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b6845ede0c4d5fa53d8b1291ded67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=277), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_ds = RTEDataset(split='dev', bert_model='bert-base-uncased', max_len=128)\n",
    "test_dl = test_ds.get_dataloader(batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.679\n",
      "Precision: 0.669\n",
      "Recall: 0.774\n",
      "F1: 0.717\n",
      "        y=1    y=2   \n",
      " l=1    113    56    \n",
      " l=2    33     75    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6787003610108303, 0.6686390532544378, 0.773972602739726, 0.7174603174603176]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test end model\n",
    "end_model.score(test_dl, metric=[\"accuracy\", \"precision\", \"recall\", \"f1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
