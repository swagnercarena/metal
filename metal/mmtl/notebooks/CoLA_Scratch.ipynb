{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import metal\n",
    "import os\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import BERTDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8551/8551 [00:02<00:00, 3990.70it/s]\n",
      "100%|██████████| 1043/1043 [00:00<00:00, 4127.75it/s]\n"
     ]
    }
   ],
   "source": [
    "model = 'bert-base-uncased' # also try bert-base-multilingual-cased (recommended)\n",
    "src_path = os.path.join(os.environ['GLUEDATA'], 'CoLA/{}.tsv')\n",
    "dataloaders = {}\n",
    "for split in ['train','dev']:\n",
    "    dataset = BERTDataset(\n",
    "        tsv_path = src_path.format(split),\n",
    "        sent1_idx=3,\n",
    "        label_idx=1,\n",
    "        skip_rows=0,\n",
    "        label_fn=lambda x: int(x)+1 # labels are scores [1, 2] (multiclass with cardinality k)\n",
    "    )\n",
    "\n",
    "    dataloaders[split] = dataset.get_dataloader(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from metal.end_model import EndModel\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertEncoder, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def forward(self, data):\n",
    "        tokens, segments, mask = data\n",
    "        # TODO: check if we should return all layers or just last hidden representation \n",
    "        _, hidden_layer = self.bert_model(tokens, segments, mask)\n",
    "        return hidden_layer\n",
    "\n",
    "class SSTHead(EndModel):     \n",
    "    def __init__(self, output_dims, **kwargs):\n",
    "        super(SSTHead, self).__init__(output_dims, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_module = BertEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_model = SSTHead(\n",
    "    [768, 2],\n",
    "    input_module=encoder_module,\n",
    "    seed=123,\n",
    "    skip_head=False,\n",
    "    input_relu=False,\n",
    "    input_batchnorm=False,\n",
    "    verbose=False,\n",
    "    device=torch.device(\"cuda\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.utils import place_on_gpu\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "def cola_metrics(model, dataloader):\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    for batch in dataloader:\n",
    "        \n",
    "        # HACK -- we won't need to do this moving forward\n",
    "        batch = place_on_gpu(batch)\n",
    "        \n",
    "        data, Y = batch\n",
    "        \n",
    "        Y = Y.detach().cpu().numpy() - 1\n",
    "\n",
    "        output = model.forward(data)\n",
    "        prediction = output.detach().cpu().numpy()\n",
    "#         import pdb; pdb.set_trace()\n",
    "\n",
    "        targets.append(Y)\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    targets = np.concatenate(targets)    \n",
    "    predictions = np.concatenate(predictions)\n",
    "    metrics = {\n",
    "        \"matthews_corr\": matthews_corr(targets, predictions)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def matthews_corr(targets, predictions):\n",
    "    predictions = np.argmax(predictions,1)\n",
    "    return matthews_corrcoef(targets, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU...\n",
      "Saving model at iteration 1 with best score 0.273\n",
      "[50 bat (0.02 epo)]: TRAIN:[loss=0.456, matthews_corr=0.496]\n",
      "[100 bat (0.04 epo)]: TRAIN:[loss=0.374, matthews_corr=0.528] VALID:[matthews_corr=0.339]\n",
      "[150 bat (0.05 epo)]: TRAIN:[loss=0.559, matthews_corr=0.530]\n",
      "[200 bat (0.07 epo)]: TRAIN:[loss=0.489, matthews_corr=0.356] VALID:[matthews_corr=0.191]\n",
      "[250 bat (0.09 epo)]: TRAIN:[loss=0.508, matthews_corr=0.602]\n",
      "Saving model at iteration 269 with best score 0.257\n",
      "[300 bat (0.11 epo)]: TRAIN:[loss=0.367, matthews_corr=0.659] VALID:[matthews_corr=0.458]\n",
      "[350 bat (0.12 epo)]: TRAIN:[loss=0.354, matthews_corr=0.643]\n",
      "Saving model at iteration 351 with best score 0.234\n",
      "Saving model at iteration 353 with best score 0.209\n",
      "Saving model at iteration 354 with best score 0.208\n",
      "[400 bat (0.14 epo)]: TRAIN:[loss=0.351, matthews_corr=0.684] VALID:[matthews_corr=0.405]\n",
      "[450 bat (0.16 epo)]: TRAIN:[loss=0.361, matthews_corr=0.316]\n",
      "[500 bat (0.18 epo)]: TRAIN:[loss=0.372, matthews_corr=0.708] VALID:[matthews_corr=0.464]\n",
      "Saving model at iteration 537 with best score 0.102\n",
      "[550 bat (0.19 epo)]: TRAIN:[loss=0.347, matthews_corr=0.777]\n",
      "Saving model at iteration 552 with best score 0.097\n",
      "[600 bat (0.21 epo)]: TRAIN:[loss=0.306, matthews_corr=0.588] VALID:[matthews_corr=0.318]\n",
      "[650 bat (0.23 epo)]: TRAIN:[loss=0.236, matthews_corr=0.708]\n",
      "[700 bat (0.25 epo)]: TRAIN:[loss=0.294, matthews_corr=0.809] VALID:[matthews_corr=0.505]\n",
      "[750 bat (0.26 epo)]: TRAIN:[loss=0.286, matthews_corr=0.746]\n",
      "[800 bat (0.28 epo)]: TRAIN:[loss=0.258, matthews_corr=0.815] VALID:[matthews_corr=0.423]\n",
      "[850 bat (0.30 epo)]: TRAIN:[loss=0.264, matthews_corr=0.830]\n",
      "[900 bat (0.32 epo)]: TRAIN:[loss=0.212, matthews_corr=0.802] VALID:[matthews_corr=0.430]\n",
      "[950 bat (0.33 epo)]: TRAIN:[loss=0.228, matthews_corr=0.810]\n",
      "[1000 bat (0.35 epo)]: TRAIN:[loss=0.201, matthews_corr=0.849] VALID:[matthews_corr=0.430]\n",
      "Saving model at iteration 1001 with best score 0.016\n",
      "[1050 bat (0.37 epo)]: TRAIN:[loss=0.209, matthews_corr=0.882]\n",
      "[1100 bat (0.39 epo)]: TRAIN:[loss=0.159, matthews_corr=0.873] VALID:[matthews_corr=0.459]\n",
      "[1150 bat (0.40 epo)]: TRAIN:[loss=0.183, matthews_corr=0.862]\n",
      "[1200 bat (0.42 epo)]: TRAIN:[loss=0.140, matthews_corr=0.868] VALID:[matthews_corr=0.472]\n",
      "[1250 bat (0.44 epo)]: TRAIN:[loss=0.239, matthews_corr=0.826]\n",
      "[1300 bat (0.46 epo)]: TRAIN:[loss=0.165, matthews_corr=0.841] VALID:[matthews_corr=0.448]\n",
      "Restoring best model from iteration 1001 with score 0.016\n",
      "Finished Training\n",
      "Accuracy: 0.771\n",
      "        y=1    y=2   \n",
      " l=1    141    58    \n",
      " l=2    181    663   \n",
      "CPU times: user 9min 18s, sys: 33.3 s, total: 9min 51s\n",
      "Wall time: 10min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "end_model.train_model(dataloaders['train'], valid_data=dataloaders['dev'],\n",
    "                      lr=0.0001, l2=0,\n",
    "                      n_epochs=5,\n",
    "                      log_train_metrics_func=[cola_metrics],\n",
    "                      log_train_metrics=[\"matthews_corr\"],\n",
    "                      log_train_every=50,\n",
    "                      log_valid_metrics_func=[cola_metrics],\n",
    "                      log_valid_metrics=[\"matthews_corr\"],\n",
    "                      log_valid_every=100,\n",
    "                      checkpoint_metric=\"model/train/loss\",#'matthews_corr',\n",
    "                      log_unit=\"batches\",\n",
    "                      checkpoint_metric_mode='min',\n",
    "                      verbose=True, progress_bar=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matthews_corr': 0.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cola_metrics(end_model, dataloaders['dev'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
