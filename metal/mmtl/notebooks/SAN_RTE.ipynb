{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import metal\n",
    "import os\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "from dataset import QQPDataset, RTEDataset, WNLIDataset, MNLIDataset, MRPCDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c09d1f1e1c4e9aa2c217daf958e12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2490), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d279ef1e90564d5aa29dfb36da8bb069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=277), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_ds = RTEDataset(split='train', bert_model='bert-base-uncased', max_len=128)\n",
    "train_dl, dev_dl = train_ds.get_dataloader(split_prop=0.8, batch_size=16)\n",
    "\n",
    "test_ds = RTEDataset(split='dev', bert_model='bert-base-uncased', max_len=128)\n",
    "test_dl = test_ds.get_dataloader(batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "bert_model = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, bert_model='bert-base-uncased', dropout=0.1, cache_dir=\".\"):\n",
    "        super(BertEncoder, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained(bert_model, cache_dir=cache_dir)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, data):\n",
    "        tokens, segments, mask = data\n",
    "        output_layer, hidden_layer = self.bert_model(tokens, segments, mask, output_all_encoded_layers=False)\n",
    "        output_layer = self.dropout(output_layer)\n",
    "        hidden_layer = self.dropout(hidden_layer)\n",
    "        return output_layer, hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageLayer(nn.Module):\n",
    "    def __init__(self, k=5):\n",
    "        super(AverageLayer, self).__init__()\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x / self.k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSelfAttn(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LinearSelfAttn, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "#         self.softmax = nn.Softmax(1)\n",
    "\n",
    "    def forward(self, x, x_mask):\n",
    "        scores = self.linear(x).view(x.size(0), x.size(1))\n",
    "        scores.data.masked_fill_(x_mask.data, -float('inf'))\n",
    "        alpha = torch.softmax(scores, 1)\n",
    "        return alpha.unsqueeze(1).bmm(x).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilinearSelfAttn(nn.Module):\n",
    "    def __init__(self, x_size, y_size):\n",
    "        super(BilinearSelfAttn, self).__init__()\n",
    "        self.linear = nn.Linear(y_size, x_size)\n",
    "#         self.softmax = nn.Softmax(1)\n",
    "\n",
    "    def forward(self, x, y, x_mask):\n",
    "        Wy = self.linear(y)\n",
    "        xWy = x.bmm(Wy.unsqueeze(2)).squeeze(2)\n",
    "        xWy.data.masked_fill_(x_mask.data, -float('inf'))\n",
    "        beta = torch.softmax(xWy, 1)\n",
    "        return beta.unsqueeze(1).bmm(x).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAN(nn.Module):\n",
    "    def __init__(self, bert_model=BertEncoder(), emb_size=100, hidden_size=100, num_classes=2, k=5):\n",
    "        super(SAN, self).__init__()\n",
    "        self.bert_model = bert_model\n",
    "        self.sent1_attn = LinearSelfAttn(input_size=emb_size)\n",
    "        self.sent2_attn = BilinearSelfAttn(emb_size, emb_size)\n",
    "        self.final_linear = nn.Linear(emb_size * 4, num_classes)\n",
    "        self.rnn = rnn = nn.GRU(emb_size, hidden_size, 1, batch_first=True)\n",
    "        self.k = k\n",
    "        self.num_classes = num_classes\n",
    "#         self.softmax = nn.Softmax(1)\n",
    "        \n",
    "#         for param in self.bert_model.parameters():\n",
    "#             param.requires_grad = False\n",
    "\n",
    "    def forward(self, X):\n",
    "        idx_matrix, seg_matrix, mask_matrix = X\n",
    "#         sent1, sent1_mask, sent2, sent2_mask = X\n",
    "\n",
    "        batch_size = idx_matrix.size(0)\n",
    "        \n",
    "        output_layer, _ = self.bert_model.forward(X)\n",
    "       \n",
    "        res = output_layer.new_zeros((batch_size, self.num_classes))\n",
    "        \n",
    "        sk = self.sent1_attn(output_layer, (1 - mask_matrix + seg_matrix).byte())\n",
    "        \n",
    "#         sk = self.sent1_attn(sent1, sent1_mask.byte())\n",
    "\n",
    "        for i in range(self.k):\n",
    "            \n",
    "            xk = self.sent2_attn(output_layer, sk, (1 - seg_matrix).byte())\n",
    "        \n",
    "#             xk = self.sent2_attn(sent2, sk, sent2_mask.byte())\n",
    "            _, sk = self.rnn(xk.unsqueeze(1), sk.unsqueeze(0))\n",
    "            sk = sk.squeeze(0)\n",
    "\n",
    "            f = torch.softmax(\n",
    "                self.final_linear(torch.cat((sk, xk, torch.abs(sk - xk), sk * xk), 1)), 1\n",
    "            )\n",
    "            res += f\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "san = SAN(emb_size = 768, hidden_size = 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.mmtl.task import Task\n",
    "\n",
    "task = Task(\"RTE\", {'train': train_dl, 'valid': dev_dl, \"test\": test_dl}, san, AverageLayer())\n",
    "# task = Task(task_name, dataloaders, BertEncoder(), task_head)\n",
    "tasks = [task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning train loop.\n",
      "Expecting a total of _approximately_ 2000 examples and 125 batches per epoch from 1 tasks.\n",
      "[0.26 epo]: TRAIN:[loss=0.702] VALID:[RTE/accuracy=0.560]\n",
      "Saving model at iteration 0.256 with best (min) score 0.702\n",
      "[0.51 epo]: TRAIN:[loss=0.683] VALID:[RTE/accuracy=0.546]\n",
      "Saving model at iteration 0.512 with best (min) score 0.683\n",
      "[0.77 epo]: TRAIN:[loss=0.675] VALID:[RTE/accuracy=0.610]\n",
      "Saving model at iteration 0.768 with best (min) score 0.675\n",
      "[1.02 epo]: TRAIN:[loss=0.650] VALID:[RTE/accuracy=0.649]\n",
      "Saving model at iteration 1.024 with best (min) score 0.650\n",
      "[1.28 epo]: TRAIN:[loss=0.555] VALID:[RTE/accuracy=0.639]\n",
      "Saving model at iteration 1.28 with best (min) score 0.555\n",
      "[1.54 epo]: TRAIN:[loss=0.575] VALID:[RTE/accuracy=0.610]\n",
      "[1.79 epo]: TRAIN:[loss=0.584] VALID:[RTE/accuracy=0.612]\n",
      "[2.05 epo]: TRAIN:[loss=0.566] VALID:[RTE/accuracy=0.624]\n",
      "[2.30 epo]: TRAIN:[loss=0.456] VALID:[RTE/accuracy=0.629]\n",
      "Saving model at iteration 2.304 with best (min) score 0.456\n",
      "[2.56 epo]: TRAIN:[loss=0.466] VALID:[RTE/accuracy=0.639]\n",
      "[2.82 epo]: TRAIN:[loss=0.551] VALID:[RTE/accuracy=0.580]\n",
      "Restoring best model from iteration 2.304 with score 0.456\n",
      "Finished Training\n",
      "{'RTE/test/accuracy': 0.6389891696750902}\n"
     ]
    }
   ],
   "source": [
    "from metal.end_model import EndModel\n",
    "from metal.mmtl.metal_model import MetalModel\n",
    "from metal.mmtl.trainer import MultitaskTrainer\n",
    "\n",
    "model = MetalModel(tasks, verbose=False)\n",
    "trainer = MultitaskTrainer()\n",
    "trainer.train_model(\n",
    "    model, \n",
    "    tasks, \n",
    "    n_epochs=3, \n",
    "    lr=5e-5,\n",
    "    progress_bar=False,\n",
    "    log_every=0.25,\n",
    "    score_every=0.25,\n",
    "    checkpoint_best=True,\n",
    "    #checkpoint_metric=task.name + \"/valid/accuracy\",\n",
    "    #checkpoint_metric_mode=\"max\",\n",
    "    verbose=True,\n",
    "#     device=\"cuda\",\n",
    ")\n",
    "\n",
    "# trainer.train_model(\n",
    "#     model,\n",
    "#     tasks,\n",
    "#     checkpoint_metric=\"model/train/loss\",\n",
    "#     n_epochs=1,\n",
    "#     progress_bar=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Network architecture:\n",
      "SAN(\n",
      "  (bert_model): BertEncoder(\n",
      "    (bert_model): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): BertLayerNorm()\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1)\n",
      "  )\n",
      "  (sent1_attn): LinearSelfAttn(\n",
      "    (linear): Linear(in_features=768, out_features=1, bias=True)\n",
      "  )\n",
      "  (sent2_attn): BilinearSelfAttn(\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (final_linear): Linear(in_features=3072, out_features=2, bias=True)\n",
      "  (rnn): GRU(768, 768, batch_first=True)\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from metal.end_model import EndModel\n",
    "end_model = EndModel(\n",
    "    [2], input_module=san, seed=123, device=\"cuda\", skip_head=True, input_relu=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU...\n",
      "[1 bat (0.00 epo)]: TRAIN:[loss=0.008] VALID:[accuracy=0.633]\n",
      "Saving model at iteration 1 with best (max) score 0.633\n",
      "[2 bat (0.00 epo)]: TRAIN:[loss=0.858] VALID:[accuracy=0.592]\n",
      "[3 bat (0.00 epo)]: TRAIN:[loss=0.990] VALID:[accuracy=0.590]\n",
      "[4 bat (0.00 epo)]: TRAIN:[loss=1.456] VALID:[accuracy=0.600]\n",
      "[5 bat (0.01 epo)]: TRAIN:[loss=0.209] VALID:[accuracy=0.637]\n",
      "Saving model at iteration 5 with best (max) score 0.637\n",
      "[6 bat (0.01 epo)]: TRAIN:[loss=0.515] VALID:[accuracy=0.641]\n",
      "Saving model at iteration 6 with best (max) score 0.641\n",
      "[7 bat (0.01 epo)]: TRAIN:[loss=0.008] VALID:[accuracy=0.637]\n",
      "[8 bat (0.01 epo)]: TRAIN:[loss=0.320] VALID:[accuracy=0.612]\n",
      "[9 bat (0.01 epo)]: TRAIN:[loss=1.517] VALID:[accuracy=0.582]\n",
      "[10 bat (0.01 epo)]: TRAIN:[loss=0.398] VALID:[accuracy=0.586]\n",
      "[11 bat (0.01 epo)]: TRAIN:[loss=1.125] VALID:[accuracy=0.580]\n",
      "[12 bat (0.01 epo)]: TRAIN:[loss=0.334] VALID:[accuracy=0.598]\n",
      "[13 bat (0.02 epo)]: TRAIN:[loss=1.017] VALID:[accuracy=0.620]\n",
      "[14 bat (0.02 epo)]: TRAIN:[loss=0.752] VALID:[accuracy=0.645]\n",
      "Saving model at iteration 14 with best (max) score 0.645\n",
      "[15 bat (0.02 epo)]: TRAIN:[loss=0.099] VALID:[accuracy=0.655]\n",
      "Saving model at iteration 15 with best (max) score 0.655\n",
      "[16 bat (0.02 epo)]: TRAIN:[loss=0.615] VALID:[accuracy=0.649]\n",
      "[17 bat (0.02 epo)]: TRAIN:[loss=0.680] VALID:[accuracy=0.641]\n",
      "[18 bat (0.02 epo)]: TRAIN:[loss=0.026] VALID:[accuracy=0.643]\n",
      "[19 bat (0.02 epo)]: TRAIN:[loss=0.318] VALID:[accuracy=0.629]\n",
      "[20 bat (0.02 epo)]: TRAIN:[loss=0.677] VALID:[accuracy=0.620]\n",
      "[21 bat (0.03 epo)]: TRAIN:[loss=0.326] VALID:[accuracy=0.616]\n",
      "[22 bat (0.03 epo)]: TRAIN:[loss=0.237] VALID:[accuracy=0.610]\n",
      "[23 bat (0.03 epo)]: TRAIN:[loss=0.874] VALID:[accuracy=0.610]\n",
      "[24 bat (0.03 epo)]: TRAIN:[loss=0.278] VALID:[accuracy=0.606]\n",
      "[25 bat (0.03 epo)]: TRAIN:[loss=0.521] VALID:[accuracy=0.627]\n",
      "[26 bat (0.03 epo)]: TRAIN:[loss=0.715] VALID:[accuracy=0.624]\n",
      "[27 bat (0.03 epo)]: TRAIN:[loss=0.804] VALID:[accuracy=0.631]\n",
      "[28 bat (0.03 epo)]: TRAIN:[loss=0.354] VALID:[accuracy=0.641]\n",
      "[29 bat (0.03 epo)]: TRAIN:[loss=0.682] VALID:[accuracy=0.637]\n",
      "[30 bat (0.04 epo)]: TRAIN:[loss=0.582] VALID:[accuracy=0.639]\n",
      "[31 bat (0.04 epo)]: TRAIN:[loss=0.358] VALID:[accuracy=0.641]\n",
      "[32 bat (0.04 epo)]: TRAIN:[loss=0.117] VALID:[accuracy=0.641]\n",
      "[33 bat (0.04 epo)]: TRAIN:[loss=0.554] VALID:[accuracy=0.643]\n",
      "[34 bat (0.04 epo)]: TRAIN:[loss=0.469] VALID:[accuracy=0.647]\n",
      "[35 bat (0.04 epo)]: TRAIN:[loss=0.092] VALID:[accuracy=0.639]\n",
      "[36 bat (0.04 epo)]: TRAIN:[loss=0.400] VALID:[accuracy=0.637]\n",
      "[37 bat (0.04 epo)]: TRAIN:[loss=0.312] VALID:[accuracy=0.647]\n",
      "[38 bat (0.05 epo)]: TRAIN:[loss=0.227] VALID:[accuracy=0.631]\n",
      "[39 bat (0.05 epo)]: TRAIN:[loss=0.198] VALID:[accuracy=0.635]\n",
      "[40 bat (0.05 epo)]: TRAIN:[loss=0.245] VALID:[accuracy=0.633]\n",
      "[41 bat (0.05 epo)]: TRAIN:[loss=0.578] VALID:[accuracy=0.639]\n",
      "[42 bat (0.05 epo)]: TRAIN:[loss=0.336] VALID:[accuracy=0.641]\n",
      "[43 bat (0.05 epo)]: TRAIN:[loss=0.525] VALID:[accuracy=0.635]\n",
      "[44 bat (0.05 epo)]: TRAIN:[loss=0.311] VALID:[accuracy=0.645]\n",
      "[45 bat (0.05 epo)]: TRAIN:[loss=0.495] VALID:[accuracy=0.643]\n",
      "[46 bat (0.06 epo)]: TRAIN:[loss=0.123] VALID:[accuracy=0.637]\n",
      "[47 bat (0.06 epo)]: TRAIN:[loss=0.371] VALID:[accuracy=0.641]\n",
      "[48 bat (0.06 epo)]: TRAIN:[loss=0.705] VALID:[accuracy=0.637]\n",
      "[49 bat (0.06 epo)]: TRAIN:[loss=0.317] VALID:[accuracy=0.633]\n",
      "[50 bat (0.06 epo)]: TRAIN:[loss=0.152] VALID:[accuracy=0.641]\n",
      "[51 bat (0.06 epo)]: TRAIN:[loss=0.146] VALID:[accuracy=0.633]\n",
      "[52 bat (0.06 epo)]: TRAIN:[loss=0.303] VALID:[accuracy=0.629]\n",
      "[53 bat (0.06 epo)]: TRAIN:[loss=0.283] VALID:[accuracy=0.627]\n",
      "[54 bat (0.07 epo)]: TRAIN:[loss=0.180] VALID:[accuracy=0.627]\n",
      "[55 bat (0.07 epo)]: TRAIN:[loss=0.414] VALID:[accuracy=0.629]\n",
      "[56 bat (0.07 epo)]: TRAIN:[loss=0.542] VALID:[accuracy=0.643]\n",
      "[57 bat (0.07 epo)]: TRAIN:[loss=0.146] VALID:[accuracy=0.649]\n",
      "[58 bat (0.07 epo)]: TRAIN:[loss=0.264] VALID:[accuracy=0.641]\n",
      "[59 bat (0.07 epo)]: TRAIN:[loss=0.681] VALID:[accuracy=0.649]\n",
      "[60 bat (0.07 epo)]: TRAIN:[loss=0.186] VALID:[accuracy=0.647]\n",
      "[61 bat (0.07 epo)]: TRAIN:[loss=0.923] VALID:[accuracy=0.639]\n",
      "[62 bat (0.07 epo)]: TRAIN:[loss=0.309] VALID:[accuracy=0.645]\n",
      "[63 bat (0.08 epo)]: TRAIN:[loss=0.117] VALID:[accuracy=0.629]\n",
      "[64 bat (0.08 epo)]: TRAIN:[loss=0.217] VALID:[accuracy=0.627]\n",
      "[65 bat (0.08 epo)]: TRAIN:[loss=0.148] VALID:[accuracy=0.629]\n",
      "[66 bat (0.08 epo)]: TRAIN:[loss=0.255] VALID:[accuracy=0.637]\n",
      "[67 bat (0.08 epo)]: TRAIN:[loss=0.339] VALID:[accuracy=0.639]\n",
      "[68 bat (0.08 epo)]: TRAIN:[loss=0.151] VALID:[accuracy=0.639]\n",
      "[69 bat (0.08 epo)]: TRAIN:[loss=0.674] VALID:[accuracy=0.627]\n",
      "[70 bat (0.08 epo)]: TRAIN:[loss=0.227] VALID:[accuracy=0.608]\n",
      "[71 bat (0.09 epo)]: TRAIN:[loss=0.433] VALID:[accuracy=0.616]\n",
      "[72 bat (0.09 epo)]: TRAIN:[loss=0.390] VALID:[accuracy=0.618]\n",
      "[73 bat (0.09 epo)]: TRAIN:[loss=0.203] VALID:[accuracy=0.614]\n",
      "[74 bat (0.09 epo)]: TRAIN:[loss=0.754] VALID:[accuracy=0.610]\n",
      "[75 bat (0.09 epo)]: TRAIN:[loss=0.530] VALID:[accuracy=0.631]\n",
      "[76 bat (0.09 epo)]: TRAIN:[loss=0.632] VALID:[accuracy=0.635]\n",
      "[77 bat (0.09 epo)]: TRAIN:[loss=0.329] VALID:[accuracy=0.639]\n",
      "[78 bat (0.09 epo)]: TRAIN:[loss=0.288] VALID:[accuracy=0.627]\n",
      "[79 bat (0.10 epo)]: TRAIN:[loss=0.091] VALID:[accuracy=0.622]\n",
      "[80 bat (0.10 epo)]: TRAIN:[loss=0.295] VALID:[accuracy=0.620]\n",
      "[81 bat (0.10 epo)]: TRAIN:[loss=0.220] VALID:[accuracy=0.612]\n",
      "[82 bat (0.10 epo)]: TRAIN:[loss=0.211] VALID:[accuracy=0.616]\n",
      "[83 bat (0.10 epo)]: TRAIN:[loss=0.157] VALID:[accuracy=0.614]\n",
      "[84 bat (0.10 epo)]: TRAIN:[loss=0.473] VALID:[accuracy=0.614]\n",
      "[85 bat (0.10 epo)]: TRAIN:[loss=0.103] VALID:[accuracy=0.620]\n",
      "[86 bat (0.10 epo)]: TRAIN:[loss=0.355] VALID:[accuracy=0.614]\n",
      "[87 bat (0.10 epo)]: TRAIN:[loss=0.266] VALID:[accuracy=0.612]\n",
      "[88 bat (0.11 epo)]: TRAIN:[loss=0.106] VALID:[accuracy=0.614]\n",
      "[89 bat (0.11 epo)]: TRAIN:[loss=0.095] VALID:[accuracy=0.620]\n",
      "[90 bat (0.11 epo)]: TRAIN:[loss=0.361] VALID:[accuracy=0.616]\n",
      "[91 bat (0.11 epo)]: TRAIN:[loss=0.383] VALID:[accuracy=0.616]\n",
      "[92 bat (0.11 epo)]: TRAIN:[loss=0.279] VALID:[accuracy=0.612]\n",
      "[93 bat (0.11 epo)]: TRAIN:[loss=0.680] VALID:[accuracy=0.604]\n",
      "[94 bat (0.11 epo)]: TRAIN:[loss=0.356] VALID:[accuracy=0.614]\n",
      "[95 bat (0.11 epo)]: TRAIN:[loss=0.787] VALID:[accuracy=0.622]\n",
      "[96 bat (0.12 epo)]: TRAIN:[loss=0.393] VALID:[accuracy=0.635]\n",
      "[97 bat (0.12 epo)]: TRAIN:[loss=0.327] VALID:[accuracy=0.620]\n",
      "[98 bat (0.12 epo)]: TRAIN:[loss=0.383] VALID:[accuracy=0.620]\n",
      "[99 bat (0.12 epo)]: TRAIN:[loss=0.821] VALID:[accuracy=0.622]\n",
      "[100 bat (0.12 epo)]: TRAIN:[loss=0.380] VALID:[accuracy=0.624]\n",
      "[101 bat (0.12 epo)]: TRAIN:[loss=0.258] VALID:[accuracy=0.631]\n",
      "[102 bat (0.12 epo)]: TRAIN:[loss=0.421] VALID:[accuracy=0.627]\n",
      "[103 bat (0.12 epo)]: TRAIN:[loss=0.213] VALID:[accuracy=0.616]\n",
      "[104 bat (0.13 epo)]: TRAIN:[loss=0.401] VALID:[accuracy=0.614]\n",
      "[105 bat (0.13 epo)]: TRAIN:[loss=0.132] VALID:[accuracy=0.610]\n",
      "[106 bat (0.13 epo)]: TRAIN:[loss=0.448] VALID:[accuracy=0.614]\n",
      "[107 bat (0.13 epo)]: TRAIN:[loss=0.343] VALID:[accuracy=0.614]\n",
      "[108 bat (0.13 epo)]: TRAIN:[loss=0.293] VALID:[accuracy=0.612]\n",
      "[109 bat (0.13 epo)]: TRAIN:[loss=0.443] VALID:[accuracy=0.618]\n",
      "[110 bat (0.13 epo)]: TRAIN:[loss=0.252] VALID:[accuracy=0.622]\n",
      "[111 bat (0.13 epo)]: TRAIN:[loss=0.282] VALID:[accuracy=0.633]\n",
      "[112 bat (0.13 epo)]: TRAIN:[loss=0.523] VALID:[accuracy=0.637]\n",
      "[113 bat (0.14 epo)]: TRAIN:[loss=0.307] VALID:[accuracy=0.618]\n",
      "[114 bat (0.14 epo)]: TRAIN:[loss=0.356] VALID:[accuracy=0.618]\n",
      "[115 bat (0.14 epo)]: TRAIN:[loss=0.210] VALID:[accuracy=0.620]\n",
      "[116 bat (0.14 epo)]: TRAIN:[loss=0.253] VALID:[accuracy=0.629]\n",
      "[117 bat (0.14 epo)]: TRAIN:[loss=0.258] VALID:[accuracy=0.629]\n",
      "[118 bat (0.14 epo)]: TRAIN:[loss=0.249] VALID:[accuracy=0.624]\n",
      "[119 bat (0.14 epo)]: TRAIN:[loss=0.559] VALID:[accuracy=0.618]\n",
      "[120 bat (0.14 epo)]: TRAIN:[loss=0.296] VALID:[accuracy=0.622]\n",
      "[121 bat (0.15 epo)]: TRAIN:[loss=0.416] VALID:[accuracy=0.627]\n",
      "[122 bat (0.15 epo)]: TRAIN:[loss=0.483] VALID:[accuracy=0.631]\n",
      "[123 bat (0.15 epo)]: TRAIN:[loss=0.178] VALID:[accuracy=0.631]\n",
      "[124 bat (0.15 epo)]: TRAIN:[loss=0.526] VALID:[accuracy=0.635]\n",
      "[125 bat (0.15 epo)]: TRAIN:[loss=0.568] VALID:[accuracy=0.637]\n",
      "[126 bat (0.15 epo)]: TRAIN:[loss=0.348] VALID:[accuracy=0.633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[127 bat (0.15 epo)]: TRAIN:[loss=0.165] VALID:[accuracy=0.635]\n",
      "[128 bat (0.15 epo)]: TRAIN:[loss=0.135] VALID:[accuracy=0.627]\n",
      "[129 bat (0.16 epo)]: TRAIN:[loss=0.222] VALID:[accuracy=0.631]\n",
      "[130 bat (0.16 epo)]: TRAIN:[loss=0.183] VALID:[accuracy=0.637]\n",
      "[131 bat (0.16 epo)]: TRAIN:[loss=0.146] VALID:[accuracy=0.635]\n",
      "[132 bat (0.16 epo)]: TRAIN:[loss=0.103] VALID:[accuracy=0.635]\n",
      "[133 bat (0.16 epo)]: TRAIN:[loss=0.195] VALID:[accuracy=0.635]\n",
      "[134 bat (0.16 epo)]: TRAIN:[loss=0.154] VALID:[accuracy=0.637]\n",
      "[135 bat (0.16 epo)]: TRAIN:[loss=0.098] VALID:[accuracy=0.639]\n",
      "[136 bat (0.16 epo)]: TRAIN:[loss=0.221] VALID:[accuracy=0.639]\n",
      "[137 bat (0.17 epo)]: TRAIN:[loss=0.298] VALID:[accuracy=0.641]\n",
      "[138 bat (0.17 epo)]: TRAIN:[loss=0.395] VALID:[accuracy=0.643]\n",
      "[139 bat (0.17 epo)]: TRAIN:[loss=0.077] VALID:[accuracy=0.641]\n",
      "[140 bat (0.17 epo)]: TRAIN:[loss=0.084] VALID:[accuracy=0.645]\n",
      "[141 bat (0.17 epo)]: TRAIN:[loss=0.076] VALID:[accuracy=0.645]\n",
      "[142 bat (0.17 epo)]: TRAIN:[loss=0.251] VALID:[accuracy=0.641]\n",
      "[143 bat (0.17 epo)]: TRAIN:[loss=0.128] VALID:[accuracy=0.635]\n",
      "[144 bat (0.17 epo)]: TRAIN:[loss=0.051] VALID:[accuracy=0.635]\n",
      "[145 bat (0.17 epo)]: TRAIN:[loss=0.261] VALID:[accuracy=0.641]\n",
      "[146 bat (0.18 epo)]: TRAIN:[loss=0.027] VALID:[accuracy=0.643]\n",
      "[147 bat (0.18 epo)]: TRAIN:[loss=0.159] VALID:[accuracy=0.641]\n",
      "[148 bat (0.18 epo)]: TRAIN:[loss=0.117] VALID:[accuracy=0.637]\n",
      "[149 bat (0.18 epo)]: TRAIN:[loss=0.368] VALID:[accuracy=0.637]\n",
      "[150 bat (0.18 epo)]: TRAIN:[loss=0.042] VALID:[accuracy=0.635]\n",
      "[151 bat (0.18 epo)]: TRAIN:[loss=0.046] VALID:[accuracy=0.639]\n",
      "[152 bat (0.18 epo)]: TRAIN:[loss=0.107] VALID:[accuracy=0.649]\n",
      "[153 bat (0.18 epo)]: TRAIN:[loss=0.446] VALID:[accuracy=0.647]\n",
      "[154 bat (0.19 epo)]: TRAIN:[loss=0.041] VALID:[accuracy=0.647]\n",
      "[155 bat (0.19 epo)]: TRAIN:[loss=0.335] VALID:[accuracy=0.651]\n",
      "[156 bat (0.19 epo)]: TRAIN:[loss=0.277] VALID:[accuracy=0.633]\n",
      "[157 bat (0.19 epo)]: TRAIN:[loss=0.212] VALID:[accuracy=0.631]\n",
      "[158 bat (0.19 epo)]: TRAIN:[loss=0.322] VALID:[accuracy=0.627]\n",
      "[159 bat (0.19 epo)]: TRAIN:[loss=0.061] VALID:[accuracy=0.627]\n",
      "[160 bat (0.19 epo)]: TRAIN:[loss=0.192] VALID:[accuracy=0.631]\n",
      "[161 bat (0.19 epo)]: TRAIN:[loss=0.371] VALID:[accuracy=0.653]\n",
      "[162 bat (0.20 epo)]: TRAIN:[loss=0.286] VALID:[accuracy=0.645]\n",
      "[163 bat (0.20 epo)]: TRAIN:[loss=0.214] VALID:[accuracy=0.639]\n",
      "[164 bat (0.20 epo)]: TRAIN:[loss=0.026] VALID:[accuracy=0.647]\n",
      "[165 bat (0.20 epo)]: TRAIN:[loss=0.175] VALID:[accuracy=0.647]\n",
      "[166 bat (0.20 epo)]: TRAIN:[loss=0.302] VALID:[accuracy=0.643]\n",
      "[167 bat (0.20 epo)]: TRAIN:[loss=0.044] VALID:[accuracy=0.639]\n",
      "[168 bat (0.20 epo)]: TRAIN:[loss=0.177] VALID:[accuracy=0.637]\n",
      "[169 bat (0.20 epo)]: TRAIN:[loss=0.136] VALID:[accuracy=0.635]\n",
      "[170 bat (0.20 epo)]: TRAIN:[loss=0.389] VALID:[accuracy=0.643]\n",
      "[171 bat (0.21 epo)]: TRAIN:[loss=0.027] VALID:[accuracy=0.641]\n",
      "[172 bat (0.21 epo)]: TRAIN:[loss=0.270] VALID:[accuracy=0.635]\n",
      "[173 bat (0.21 epo)]: TRAIN:[loss=0.069] VALID:[accuracy=0.635]\n",
      "[174 bat (0.21 epo)]: TRAIN:[loss=0.100] VALID:[accuracy=0.653]\n",
      "[175 bat (0.21 epo)]: TRAIN:[loss=0.305] VALID:[accuracy=0.657]\n",
      "Saving model at iteration 175 with best (max) score 0.657\n",
      "[176 bat (0.21 epo)]: TRAIN:[loss=0.162] VALID:[accuracy=0.669]\n",
      "Saving model at iteration 176 with best (max) score 0.669\n",
      "[177 bat (0.21 epo)]: TRAIN:[loss=0.135] VALID:[accuracy=0.671]\n",
      "Saving model at iteration 177 with best (max) score 0.671\n",
      "[178 bat (0.21 epo)]: TRAIN:[loss=0.052] VALID:[accuracy=0.657]\n",
      "[179 bat (0.22 epo)]: TRAIN:[loss=0.062] VALID:[accuracy=0.647]\n",
      "[180 bat (0.22 epo)]: TRAIN:[loss=0.117] VALID:[accuracy=0.631]\n",
      "[181 bat (0.22 epo)]: TRAIN:[loss=0.057] VALID:[accuracy=0.629]\n",
      "[182 bat (0.22 epo)]: TRAIN:[loss=0.306] VALID:[accuracy=0.631]\n",
      "[183 bat (0.22 epo)]: TRAIN:[loss=0.203] VALID:[accuracy=0.633]\n",
      "[184 bat (0.22 epo)]: TRAIN:[loss=0.043] VALID:[accuracy=0.631]\n",
      "[185 bat (0.22 epo)]: TRAIN:[loss=0.048] VALID:[accuracy=0.629]\n",
      "[186 bat (0.22 epo)]: TRAIN:[loss=0.035] VALID:[accuracy=0.629]\n",
      "[187 bat (0.23 epo)]: TRAIN:[loss=0.091] VALID:[accuracy=0.631]\n",
      "[188 bat (0.23 epo)]: TRAIN:[loss=0.333] VALID:[accuracy=0.631]\n",
      "[189 bat (0.23 epo)]: TRAIN:[loss=0.312] VALID:[accuracy=0.633]\n",
      "[190 bat (0.23 epo)]: TRAIN:[loss=0.091] VALID:[accuracy=0.641]\n",
      "[191 bat (0.23 epo)]: TRAIN:[loss=0.148] VALID:[accuracy=0.649]\n",
      "[192 bat (0.23 epo)]: TRAIN:[loss=0.193] VALID:[accuracy=0.653]\n",
      "[193 bat (0.23 epo)]: TRAIN:[loss=0.213] VALID:[accuracy=0.657]\n",
      "[194 bat (0.23 epo)]: TRAIN:[loss=0.096] VALID:[accuracy=0.659]\n",
      "[195 bat (0.23 epo)]: TRAIN:[loss=0.258] VALID:[accuracy=0.651]\n",
      "[196 bat (0.24 epo)]: TRAIN:[loss=0.220] VALID:[accuracy=0.647]\n",
      "[197 bat (0.24 epo)]: TRAIN:[loss=0.100] VALID:[accuracy=0.651]\n",
      "[198 bat (0.24 epo)]: TRAIN:[loss=0.325] VALID:[accuracy=0.643]\n",
      "[199 bat (0.24 epo)]: TRAIN:[loss=0.041] VALID:[accuracy=0.639]\n",
      "[200 bat (0.24 epo)]: TRAIN:[loss=0.050] VALID:[accuracy=0.637]\n",
      "[201 bat (0.24 epo)]: TRAIN:[loss=0.177] VALID:[accuracy=0.629]\n",
      "[202 bat (0.24 epo)]: TRAIN:[loss=0.295] VALID:[accuracy=0.635]\n",
      "[203 bat (0.24 epo)]: TRAIN:[loss=0.081] VALID:[accuracy=0.633]\n",
      "[204 bat (0.25 epo)]: TRAIN:[loss=0.211] VALID:[accuracy=0.639]\n",
      "[205 bat (0.25 epo)]: TRAIN:[loss=0.060] VALID:[accuracy=0.645]\n",
      "[206 bat (0.25 epo)]: TRAIN:[loss=0.125] VALID:[accuracy=0.643]\n",
      "[207 bat (0.25 epo)]: TRAIN:[loss=0.030] VALID:[accuracy=0.635]\n",
      "[208 bat (0.25 epo)]: TRAIN:[loss=0.185] VALID:[accuracy=0.624]\n",
      "[209 bat (0.25 epo)]: TRAIN:[loss=0.153] VALID:[accuracy=0.627]\n",
      "[210 bat (0.25 epo)]: TRAIN:[loss=0.191] VALID:[accuracy=0.649]\n",
      "[211 bat (0.25 epo)]: TRAIN:[loss=0.147] VALID:[accuracy=0.645]\n",
      "[212 bat (0.26 epo)]: TRAIN:[loss=0.476] VALID:[accuracy=0.643]\n",
      "[213 bat (0.26 epo)]: TRAIN:[loss=0.368] VALID:[accuracy=0.637]\n",
      "[214 bat (0.26 epo)]: TRAIN:[loss=0.116] VALID:[accuracy=0.637]\n",
      "[215 bat (0.26 epo)]: TRAIN:[loss=0.790] VALID:[accuracy=0.641]\n",
      "[216 bat (0.26 epo)]: TRAIN:[loss=0.141] VALID:[accuracy=0.641]\n",
      "[217 bat (0.26 epo)]: TRAIN:[loss=0.072] VALID:[accuracy=0.639]\n",
      "[218 bat (0.26 epo)]: TRAIN:[loss=0.191] VALID:[accuracy=0.643]\n",
      "[219 bat (0.26 epo)]: TRAIN:[loss=0.305] VALID:[accuracy=0.639]\n",
      "[220 bat (0.27 epo)]: TRAIN:[loss=0.390] VALID:[accuracy=0.635]\n",
      "[221 bat (0.27 epo)]: TRAIN:[loss=0.075] VALID:[accuracy=0.635]\n",
      "[222 bat (0.27 epo)]: TRAIN:[loss=0.048] VALID:[accuracy=0.635]\n",
      "[223 bat (0.27 epo)]: TRAIN:[loss=0.441] VALID:[accuracy=0.635]\n",
      "[224 bat (0.27 epo)]: TRAIN:[loss=0.095] VALID:[accuracy=0.639]\n",
      "[225 bat (0.27 epo)]: TRAIN:[loss=0.101] VALID:[accuracy=0.643]\n",
      "[226 bat (0.27 epo)]: TRAIN:[loss=0.096] VALID:[accuracy=0.647]\n",
      "[227 bat (0.27 epo)]: TRAIN:[loss=0.263] VALID:[accuracy=0.649]\n",
      "[228 bat (0.27 epo)]: TRAIN:[loss=0.058] VALID:[accuracy=0.649]\n",
      "[229 bat (0.28 epo)]: TRAIN:[loss=0.550] VALID:[accuracy=0.647]\n",
      "[230 bat (0.28 epo)]: TRAIN:[loss=0.158] VALID:[accuracy=0.635]\n",
      "[231 bat (0.28 epo)]: TRAIN:[loss=0.062] VALID:[accuracy=0.629]\n",
      "[232 bat (0.28 epo)]: TRAIN:[loss=0.255] VALID:[accuracy=0.635]\n",
      "[233 bat (0.28 epo)]: TRAIN:[loss=0.119] VALID:[accuracy=0.637]\n",
      "[234 bat (0.28 epo)]: TRAIN:[loss=0.368] VALID:[accuracy=0.631]\n",
      "[235 bat (0.28 epo)]: TRAIN:[loss=0.549] VALID:[accuracy=0.635]\n",
      "[236 bat (0.28 epo)]: TRAIN:[loss=0.144] VALID:[accuracy=0.635]\n",
      "[237 bat (0.29 epo)]: TRAIN:[loss=0.505] VALID:[accuracy=0.651]\n",
      "[238 bat (0.29 epo)]: TRAIN:[loss=0.309] VALID:[accuracy=0.635]\n",
      "[239 bat (0.29 epo)]: TRAIN:[loss=0.104] VALID:[accuracy=0.622]\n",
      "[240 bat (0.29 epo)]: TRAIN:[loss=0.450] VALID:[accuracy=0.622]\n",
      "[241 bat (0.29 epo)]: TRAIN:[loss=0.402] VALID:[accuracy=0.622]\n",
      "[242 bat (0.29 epo)]: TRAIN:[loss=0.117] VALID:[accuracy=0.627]\n",
      "[243 bat (0.29 epo)]: TRAIN:[loss=0.350] VALID:[accuracy=0.639]\n",
      "[244 bat (0.29 epo)]: TRAIN:[loss=0.123] VALID:[accuracy=0.633]\n",
      "[245 bat (0.30 epo)]: TRAIN:[loss=0.135] VALID:[accuracy=0.637]\n",
      "[246 bat (0.30 epo)]: TRAIN:[loss=0.360] VALID:[accuracy=0.631]\n",
      "[247 bat (0.30 epo)]: TRAIN:[loss=0.187] VALID:[accuracy=0.631]\n",
      "[248 bat (0.30 epo)]: TRAIN:[loss=0.277] VALID:[accuracy=0.635]\n",
      "[249 bat (0.30 epo)]: TRAIN:[loss=0.186] VALID:[accuracy=0.631]\n",
      "[250 bat (0.30 epo)]: TRAIN:[loss=0.070] VALID:[accuracy=0.629]\n",
      "[251 bat (0.30 epo)]: TRAIN:[loss=0.288] VALID:[accuracy=0.627]\n",
      "[252 bat (0.30 epo)]: TRAIN:[loss=0.085] VALID:[accuracy=0.627]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[253 bat (0.30 epo)]: TRAIN:[loss=0.045] VALID:[accuracy=0.637]\n",
      "[254 bat (0.31 epo)]: TRAIN:[loss=0.045] VALID:[accuracy=0.645]\n",
      "[255 bat (0.31 epo)]: TRAIN:[loss=0.077] VALID:[accuracy=0.645]\n",
      "[256 bat (0.31 epo)]: TRAIN:[loss=0.064] VALID:[accuracy=0.645]\n",
      "[257 bat (0.31 epo)]: TRAIN:[loss=0.226] VALID:[accuracy=0.647]\n",
      "[258 bat (0.31 epo)]: TRAIN:[loss=0.261] VALID:[accuracy=0.643]\n",
      "[259 bat (0.31 epo)]: TRAIN:[loss=0.267] VALID:[accuracy=0.639]\n",
      "[260 bat (0.31 epo)]: TRAIN:[loss=0.102] VALID:[accuracy=0.637]\n",
      "[261 bat (0.31 epo)]: TRAIN:[loss=0.054] VALID:[accuracy=0.631]\n",
      "[262 bat (0.32 epo)]: TRAIN:[loss=0.068] VALID:[accuracy=0.635]\n",
      "[263 bat (0.32 epo)]: TRAIN:[loss=0.115] VALID:[accuracy=0.633]\n",
      "[264 bat (0.32 epo)]: TRAIN:[loss=0.068] VALID:[accuracy=0.637]\n",
      "[265 bat (0.32 epo)]: TRAIN:[loss=0.070] VALID:[accuracy=0.637]\n",
      "[266 bat (0.32 epo)]: TRAIN:[loss=0.017] VALID:[accuracy=0.641]\n",
      "[267 bat (0.32 epo)]: TRAIN:[loss=0.259] VALID:[accuracy=0.635]\n",
      "[268 bat (0.32 epo)]: TRAIN:[loss=0.264] VALID:[accuracy=0.643]\n",
      "[269 bat (0.32 epo)]: TRAIN:[loss=0.321] VALID:[accuracy=0.641]\n",
      "[270 bat (0.33 epo)]: TRAIN:[loss=0.021] VALID:[accuracy=0.639]\n",
      "[271 bat (0.33 epo)]: TRAIN:[loss=0.023] VALID:[accuracy=0.641]\n",
      "[272 bat (0.33 epo)]: TRAIN:[loss=0.194] VALID:[accuracy=0.641]\n",
      "[273 bat (0.33 epo)]: TRAIN:[loss=0.018] VALID:[accuracy=0.639]\n",
      "[274 bat (0.33 epo)]: TRAIN:[loss=0.234] VALID:[accuracy=0.647]\n",
      "[275 bat (0.33 epo)]: TRAIN:[loss=0.268] VALID:[accuracy=0.645]\n",
      "[276 bat (0.33 epo)]: TRAIN:[loss=0.175] VALID:[accuracy=0.629]\n",
      "[277 bat (0.33 epo)]: TRAIN:[loss=0.026] VALID:[accuracy=0.622]\n",
      "[278 bat (0.33 epo)]: TRAIN:[loss=0.213] VALID:[accuracy=0.618]\n",
      "[279 bat (0.34 epo)]: TRAIN:[loss=0.070] VALID:[accuracy=0.624]\n",
      "[280 bat (0.34 epo)]: TRAIN:[loss=0.140] VALID:[accuracy=0.629]\n",
      "[281 bat (0.34 epo)]: TRAIN:[loss=0.075] VALID:[accuracy=0.647]\n",
      "[282 bat (0.34 epo)]: TRAIN:[loss=0.093] VALID:[accuracy=0.637]\n",
      "[283 bat (0.34 epo)]: TRAIN:[loss=0.062] VALID:[accuracy=0.633]\n",
      "[284 bat (0.34 epo)]: TRAIN:[loss=0.199] VALID:[accuracy=0.624]\n",
      "[285 bat (0.34 epo)]: TRAIN:[loss=0.044] VALID:[accuracy=0.620]\n",
      "[286 bat (0.34 epo)]: TRAIN:[loss=0.110] VALID:[accuracy=0.631]\n",
      "[287 bat (0.35 epo)]: TRAIN:[loss=0.017] VALID:[accuracy=0.622]\n",
      "[288 bat (0.35 epo)]: TRAIN:[loss=0.220] VALID:[accuracy=0.618]\n",
      "[289 bat (0.35 epo)]: TRAIN:[loss=0.014] VALID:[accuracy=0.618]\n",
      "[290 bat (0.35 epo)]: TRAIN:[loss=0.448] VALID:[accuracy=0.618]\n",
      "[291 bat (0.35 epo)]: TRAIN:[loss=0.050] VALID:[accuracy=0.618]\n",
      "[292 bat (0.35 epo)]: TRAIN:[loss=0.029] VALID:[accuracy=0.622]\n",
      "[293 bat (0.35 epo)]: TRAIN:[loss=0.147] VALID:[accuracy=0.627]\n",
      "[294 bat (0.35 epo)]: TRAIN:[loss=0.065] VALID:[accuracy=0.643]\n",
      "[295 bat (0.36 epo)]: TRAIN:[loss=0.039] VALID:[accuracy=0.643]\n",
      "[296 bat (0.36 epo)]: TRAIN:[loss=0.012] VALID:[accuracy=0.641]\n",
      "[297 bat (0.36 epo)]: TRAIN:[loss=0.187] VALID:[accuracy=0.647]\n",
      "[298 bat (0.36 epo)]: TRAIN:[loss=0.043] VALID:[accuracy=0.647]\n",
      "[299 bat (0.36 epo)]: TRAIN:[loss=0.053] VALID:[accuracy=0.649]\n",
      "[300 bat (0.36 epo)]: TRAIN:[loss=0.148] VALID:[accuracy=0.649]\n",
      "[301 bat (0.36 epo)]: TRAIN:[loss=0.122] VALID:[accuracy=0.627]\n",
      "[302 bat (0.36 epo)]: TRAIN:[loss=0.016] VALID:[accuracy=0.627]\n",
      "[303 bat (0.37 epo)]: TRAIN:[loss=0.120] VALID:[accuracy=0.631]\n",
      "[304 bat (0.37 epo)]: TRAIN:[loss=0.019] VALID:[accuracy=0.633]\n",
      "[305 bat (0.37 epo)]: TRAIN:[loss=0.042] VALID:[accuracy=0.622]\n",
      "[306 bat (0.37 epo)]: TRAIN:[loss=0.333] VALID:[accuracy=0.622]\n",
      "[307 bat (0.37 epo)]: TRAIN:[loss=0.034] VALID:[accuracy=0.624]\n",
      "[308 bat (0.37 epo)]: TRAIN:[loss=0.085] VALID:[accuracy=0.622]\n",
      "[309 bat (0.37 epo)]: TRAIN:[loss=0.076] VALID:[accuracy=0.624]\n",
      "[310 bat (0.37 epo)]: TRAIN:[loss=0.010] VALID:[accuracy=0.624]\n",
      "[311 bat (0.37 epo)]: TRAIN:[loss=0.026] VALID:[accuracy=0.627]\n",
      "[312 bat (0.38 epo)]: TRAIN:[loss=0.016] VALID:[accuracy=0.629]\n",
      "[313 bat (0.38 epo)]: TRAIN:[loss=0.016] VALID:[accuracy=0.627]\n",
      "[314 bat (0.38 epo)]: TRAIN:[loss=0.012] VALID:[accuracy=0.631]\n",
      "[315 bat (0.38 epo)]: TRAIN:[loss=0.104] VALID:[accuracy=0.637]\n",
      "[316 bat (0.38 epo)]: TRAIN:[loss=0.292] VALID:[accuracy=0.645]\n",
      "[317 bat (0.38 epo)]: TRAIN:[loss=0.020] VALID:[accuracy=0.627]\n",
      "[318 bat (0.38 epo)]: TRAIN:[loss=0.066] VALID:[accuracy=0.618]\n",
      "[319 bat (0.38 epo)]: TRAIN:[loss=0.082] VALID:[accuracy=0.620]\n",
      "[320 bat (0.39 epo)]: TRAIN:[loss=0.029] VALID:[accuracy=0.620]\n",
      "[321 bat (0.39 epo)]: TRAIN:[loss=0.240] VALID:[accuracy=0.639]\n",
      "[322 bat (0.39 epo)]: TRAIN:[loss=0.061] VALID:[accuracy=0.639]\n",
      "[323 bat (0.39 epo)]: TRAIN:[loss=0.390] VALID:[accuracy=0.637]\n",
      "[324 bat (0.39 epo)]: TRAIN:[loss=0.128] VALID:[accuracy=0.637]\n",
      "[325 bat (0.39 epo)]: TRAIN:[loss=0.247] VALID:[accuracy=0.635]\n",
      "[326 bat (0.39 epo)]: TRAIN:[loss=0.030] VALID:[accuracy=0.633]\n",
      "[327 bat (0.39 epo)]: TRAIN:[loss=0.094] VALID:[accuracy=0.633]\n",
      "[328 bat (0.40 epo)]: TRAIN:[loss=0.287] VALID:[accuracy=0.635]\n",
      "[329 bat (0.40 epo)]: TRAIN:[loss=0.070] VALID:[accuracy=0.635]\n",
      "[330 bat (0.40 epo)]: TRAIN:[loss=0.056] VALID:[accuracy=0.635]\n",
      "[331 bat (0.40 epo)]: TRAIN:[loss=0.043] VALID:[accuracy=0.641]\n",
      "[332 bat (0.40 epo)]: TRAIN:[loss=0.016] VALID:[accuracy=0.641]\n",
      "[333 bat (0.40 epo)]: TRAIN:[loss=0.032] VALID:[accuracy=0.639]\n",
      "[334 bat (0.40 epo)]: TRAIN:[loss=0.106] VALID:[accuracy=0.637]\n",
      "[335 bat (0.40 epo)]: TRAIN:[loss=0.022] VALID:[accuracy=0.629]\n",
      "[336 bat (0.40 epo)]: TRAIN:[loss=0.028] VALID:[accuracy=0.631]\n",
      "[337 bat (0.41 epo)]: TRAIN:[loss=0.024] VALID:[accuracy=0.633]\n",
      "[338 bat (0.41 epo)]: TRAIN:[loss=0.083] VALID:[accuracy=0.624]\n",
      "[339 bat (0.41 epo)]: TRAIN:[loss=0.189] VALID:[accuracy=0.627]\n",
      "[340 bat (0.41 epo)]: TRAIN:[loss=0.027] VALID:[accuracy=0.639]\n",
      "[341 bat (0.41 epo)]: TRAIN:[loss=0.037] VALID:[accuracy=0.639]\n",
      "[342 bat (0.41 epo)]: TRAIN:[loss=0.031] VALID:[accuracy=0.629]\n",
      "[343 bat (0.41 epo)]: TRAIN:[loss=0.102] VALID:[accuracy=0.627]\n",
      "[344 bat (0.41 epo)]: TRAIN:[loss=0.140] VALID:[accuracy=0.641]\n",
      "[345 bat (0.42 epo)]: TRAIN:[loss=0.047] VALID:[accuracy=0.631]\n",
      "[346 bat (0.42 epo)]: TRAIN:[loss=0.121] VALID:[accuracy=0.622]\n",
      "[347 bat (0.42 epo)]: TRAIN:[loss=0.375] VALID:[accuracy=0.622]\n",
      "[348 bat (0.42 epo)]: TRAIN:[loss=0.132] VALID:[accuracy=0.604]\n",
      "[349 bat (0.42 epo)]: TRAIN:[loss=0.052] VALID:[accuracy=0.614]\n",
      "[350 bat (0.42 epo)]: TRAIN:[loss=0.046] VALID:[accuracy=0.604]\n",
      "[351 bat (0.42 epo)]: TRAIN:[loss=0.295] VALID:[accuracy=0.606]\n",
      "[352 bat (0.42 epo)]: TRAIN:[loss=0.281] VALID:[accuracy=0.610]\n",
      "[353 bat (0.43 epo)]: TRAIN:[loss=0.600] VALID:[accuracy=0.610]\n",
      "[354 bat (0.43 epo)]: TRAIN:[loss=0.041] VALID:[accuracy=0.614]\n",
      "[355 bat (0.43 epo)]: TRAIN:[loss=0.012] VALID:[accuracy=0.610]\n",
      "[356 bat (0.43 epo)]: TRAIN:[loss=0.036] VALID:[accuracy=0.612]\n",
      "[357 bat (0.43 epo)]: TRAIN:[loss=0.321] VALID:[accuracy=0.614]\n",
      "[358 bat (0.43 epo)]: TRAIN:[loss=0.015] VALID:[accuracy=0.620]\n",
      "[359 bat (0.43 epo)]: TRAIN:[loss=0.333] VALID:[accuracy=0.618]\n",
      "[360 bat (0.43 epo)]: TRAIN:[loss=0.013] VALID:[accuracy=0.622]\n",
      "[361 bat (0.43 epo)]: TRAIN:[loss=0.024] VALID:[accuracy=0.622]\n",
      "[362 bat (0.44 epo)]: TRAIN:[loss=0.021] VALID:[accuracy=0.622]\n",
      "[363 bat (0.44 epo)]: TRAIN:[loss=0.033] VALID:[accuracy=0.620]\n",
      "[364 bat (0.44 epo)]: TRAIN:[loss=0.220] VALID:[accuracy=0.627]\n",
      "[365 bat (0.44 epo)]: TRAIN:[loss=0.307] VALID:[accuracy=0.629]\n",
      "[366 bat (0.44 epo)]: TRAIN:[loss=0.206] VALID:[accuracy=0.635]\n",
      "[367 bat (0.44 epo)]: TRAIN:[loss=0.098] VALID:[accuracy=0.624]\n",
      "[368 bat (0.44 epo)]: TRAIN:[loss=0.224] VALID:[accuracy=0.627]\n",
      "[369 bat (0.44 epo)]: TRAIN:[loss=0.083] VALID:[accuracy=0.624]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object tqdm_notebook.__iter__ at 0x7f9f213e8ba0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/dfs/scratch0/vschen/venv-mmtl/lib/python3.6/site-packages/tqdm/_tqdm_notebook.py\", line 226, in __iter__\n",
      "    self.sp(bar_style='danger')\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'sp'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-18316e46d680>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mcheckpoint_metric_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m )\n",
      "\u001b[0;32m/dfs/scratch1/senwu/mmtl/metal/metal/end_model/end_model.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, train_data, valid_data, log_writer, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# Execute training procedure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         self._train_model(\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_writer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_writer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         )\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/scratch1/senwu/mmtl/metal/metal/classifier.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, train_data, loss_fn, valid_data, log_writer, restore_state)\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0;31m# Calculate metrics, log, and checkpoint as necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                 metrics_dict = self._execute_logging(\n\u001b[0;32m--> 258\u001b[0;31m                     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m                 )\n\u001b[1;32m    260\u001b[0m                 \u001b[0mmetrics_hist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/scratch1/senwu/mmtl/metal/metal/classifier.py\u001b[0m in \u001b[0;36m_execute_logging\u001b[0;34m(self, train_loader, valid_loader, loss, batch_size)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m             logger_metrics = self.logger.calculate_metrics(\n\u001b[0;32m--> 538\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m             )\n\u001b[1;32m    540\u001b[0m             \u001b[0mmetrics_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/scratch1/senwu/mmtl/metal/metal/logging/logger.py\u001b[0m in \u001b[0;36mcalculate_metrics\u001b[0;34m(self, model, train_loader, valid_loader, metrics_dict)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlog_valid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             metrics_dict = self._calculate_standard_metrics(\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_valid_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"valid\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             )\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/scratch1/senwu/mmtl/metal/metal/logging/logger.py\u001b[0m in \u001b[0;36m_calculate_standard_metrics\u001b[0;34m(self, model, data_loader, target_metrics, metrics_dict, split)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# for all metrics calculations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 Y_preds, Y, Y_probs = model._get_predictions(\n\u001b[0;32m--> 134\u001b[0;31m                     \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_probs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m                 )\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_standard_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/scratch1/senwu/mmtl/metal/metal/classifier.py\u001b[0m in \u001b[0;36m_get_predictions\u001b[0;34m(self, data, break_ties, return_probs, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# Append predictions and labels from DataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             Y_pb, Y_sb = self.predict(\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mXb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbreak_ties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbreak_ties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_probs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             )\n\u001b[1;32m    595\u001b[0m             \u001b[0mY_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/scratch1/senwu/mmtl/metal/metal/classifier.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, break_ties, return_probs, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mOptionally\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mY_s\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mY_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mY_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_break_ties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbreak_ties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_probs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/scratch1/senwu/mmtl/metal/metal/end_model/end_model.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;34m\"\"\"Returns a [n, k] tensor of probs (probabilistic labels).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/dfs/scratch1/senwu/mmtl/metal/metal/end_model/end_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \"\"\"\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/scratch0/vschen/venv-mmtl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-0bc32cc075c2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0moutput_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-9e1f04886a8e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutput_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0moutput_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mhidden_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/scratch0/vschen/venv-mmtl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/scratch0/vschen/venv-mmtl/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    627\u001b[0m         encoded_layers = self.encoder(embedding_output,\n\u001b[1;32m    628\u001b[0m                                       \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m                                       output_all_encoded_layers=output_all_encoded_layers)\n\u001b[0m\u001b[1;32m    630\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/scratch0/vschen/venv-mmtl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/scratch0/vschen/venv-mmtl/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0mall_encoder_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_module\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0mall_encoder_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/scratch0/vschen/venv-mmtl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/scratch0/vschen/venv-mmtl/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/scratch0/vschen/venv-mmtl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/scratch0/vschen/venv-mmtl/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor, attention_mask)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mself_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/scratch0/vschen/venv-mmtl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/scratch0/vschen/venv-mmtl/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;31m# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "end_model.train_model(\n",
    "    train_dl,\n",
    "#     dataset[\"train\"].get_dataloader(batch_size=32),\n",
    "    valid_data=dev_dl,\n",
    "#     valid_data=dataset[\"dev\"].get_dataloader(batch_size=32),\n",
    "#     dataloaders[\"train\"],\n",
    "#     valid_data=dataloaders[\"dev\"],\n",
    "    lr=5e-5,\n",
    "    l2=0,\n",
    "    n_epochs=3,\n",
    "#     checkpoint_metric=\"model/train/loss\",\n",
    "    checkpoint_metric=\"valid/accuracy\",\n",
    "    log_unit=\"batches\",\n",
    "    checkpoint_metric_mode=\"max\",\n",
    "    verbose=True,\n",
    "    progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
